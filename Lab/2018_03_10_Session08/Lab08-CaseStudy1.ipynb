{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab08 Case Study 1\n",
    "\n",
    "## Towards a modern Back Propagation Implementation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 784) (7000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "#Load MNIST datset \n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, Y = mnist.data, mnist.target\n",
    "Y = Y.astype(int)\n",
    "\n",
    "X = X[::10, :]     ## taking the whole data will take a lot of processing time\n",
    "Y = Y[::10]\n",
    "# digits = datasets.load_digits(n_class=10)\n",
    "# # Create our X and y data\n",
    "# X = digits.data\n",
    "# Y = digits.target\n",
    "print(X.shape, Y.shape)\n",
    "num_examples = X.shape[0]      ## training set size\n",
    "nn_input_dim = X.shape[1]      ## input layer dimensionality\n",
    "nn_output_dim = len(np.unique(Y))       ## output layer dimensionality\n",
    "\n",
    "params = {\n",
    "    \"lr\":0.0001,        ## learning_rate\n",
    "    \"max_iter\":500,\n",
    "    \"weight_init\":\"xavier\",\n",
    "    \"h_dimn\":100,     ## hidden_layer_size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Weight Initializations\n",
    "Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative.\n",
    "\n",
    "Zero Weight Initialization: This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.\n",
    "\n",
    "As a solution, it is common to initialize the weights of the neurons to small numbers (random or unique) and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. Instead of using random initializations, it is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.\n",
    "\n",
    "It is worth mentioning that if you do not know which technique should be chosen as weight initilalizaion method, Xaiver is often choosen as a initial try.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out):\n",
    "    ## using FanAvg variation\n",
    "    n = (fan_in+fan_out)/2\n",
    "    limit = np.sqrt(3.0 * 1 / n)\n",
    "    return np.random.uniform(size = (fan_in, fan_out), low = -limit, high = +limit)\n",
    "\n",
    "def weight_initialization(params):\n",
    "    hdim = params[\"h_dimn\"]\n",
    "    winit = params[\"weight_init\"]\n",
    "    if winit == \"random\":\n",
    "        np.random.seed(0)\n",
    "        W1 = np.random.randn(nn_input_dim, hdim)\n",
    "        b1 = np.random.randn(1, hdim)\n",
    "        W2 = np.random.randn(hdim, nn_output_dim)\n",
    "        b2 = np.random.randn(1, nn_output_dim)\n",
    "    elif winit == \"zeros\":\n",
    "        W1 = np.zeros((nn_input_dim, hdim))\n",
    "        b1 = np.zeros((1, hdim))\n",
    "        W2 = np.zeros((hdim, nn_output_dim))\n",
    "        b2 = np.zeros((1, nn_output_dim))\n",
    "    elif winit == \"xavier\":\n",
    "        W1 = xavier_init(nn_input_dim, hdim)\n",
    "        b1 = xavier_init(1, hdim)\n",
    "        W2 = xavier_init(hdim, nn_output_dim)\n",
    "        b2 = xavier_init(1, nn_output_dim)\n",
    "    elif winit == \"uniform\":\n",
    "        W1 = np.random.uniform(size=(nn_input_dim, hdim), low=-1, high=1)/np.sqrt(nn_input_dim)\n",
    "        b1 = np.random.uniform(size=(1, hdim), low=-1, high=1)\n",
    "        W2 = np.random.uniform(size=(hdim, nn_output_dim), low=-1, high=1)/np.sqrt(hdim)\n",
    "        b2 = np.random.uniform(size=(1, nn_output_dim), low=-1, high=1)\n",
    "    elif winit == \"normal\":\n",
    "        W1 = np.random.normal(loc = 0, scale = 0.5, size = (nn_input_dim, hdim))\n",
    "        b1 = np.random.normal(loc = 0, scale = 0.5, size=(1, hdim))\n",
    "        W2 = np.random.normal(loc = 0, scale = 0.5, size = (hdim, nn_output_dim))\n",
    "        b2 = np.random.normal(loc = 0, scale = 0.5, size=(1, nn_output_dim))\n",
    "    return W1, b1, W2, b2 \n",
    "\n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "def build_model():\n",
    "    W1, b1, W2, b2 = weight_initialization(params)\n",
    "    # This is what we return at the end\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return model\n",
    "\n",
    "def feedforward(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    probs = softmax(z2)\n",
    "    return a1, probs\n",
    "\n",
    "def backpropagation(model, x, y, a1, probs):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    delta3 = probs\n",
    "    delta3[range(y.shape[0]), y] -= 1\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "    dW1 = np.dot(x.T, delta2)\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    return dW2, db2, dW1, db1\n",
    "\n",
    "def calculate_loss(model, x, y):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # Forward propagation to calculate predictions\n",
    "    _, probs = feedforward(model, x)\n",
    "    \n",
    "    # Calculating the cross entropy loss\n",
    "    corect_logprobs = -np.log(probs[range(y.shape[0]), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    \n",
    "    return 1./y.shape[0] * data_loss\n",
    "\n",
    "def test(model, x, y):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation to calculate predictions\n",
    "    _, probs = feedforward(model, x)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    return np.count_nonzero(y==preds)/y.shape[0]\n",
    "\n",
    "def train(model, X_train, X_test, Y_train, Y_test, verbose=True):\n",
    "    # Gradient descent. For each batch...\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    for i in range(0, params[\"max_iter\"]):\n",
    "\n",
    "        # Forward propagation\n",
    "        a1, probs = feedforward(model, X_train)\n",
    "\n",
    "        # Backpropagation\n",
    "        dW2, db2, dW1, db1 = backpropagation(model, X_train, Y_train, a1, probs)\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -params[\"lr\"] * dW1\n",
    "        b1 += -params[\"lr\"] * db1\n",
    "        W2 += -params[\"lr\"] * dW2\n",
    "        b2 += -params[\"lr\"] * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        if verbose and i % 50 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model, X_train, Y_train)),\n",
    "                  \", Test accuracy:\", test(model, X_test, Y_test), \"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Experiment with different Weight Initializations and evaluate the corresponding test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'xavier', 'h_dimn': 100} TestAccuracy= 0.9092857142857143\n",
      "{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'uniform', 'h_dimn': 100} TestAccuracy= 0.9103571428571429\n",
      "{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'normal', 'h_dimn': 100} TestAccuracy= 0.7903571428571429\n",
      "{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'zeros', 'h_dimn': 100} TestAccuracy= 0.11607142857142858\n",
      "{'lr': 0.0001, 'max_iter': 500, 'weight_init': 'random', 'h_dimn': 100} TestAccuracy= 0.7257142857142858\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4)\n",
    "t = [\"xavier\",\"uniform\",\"normal\",\"zeros\",\"random\"]\n",
    "\n",
    "for i in range(5):\n",
    "    params[\"weight_init\"] = t[i]\n",
    "    model = build_model()\n",
    "    model = train(model, X_train, X_test, Y_train, Y_test, verbose=False)\n",
    "    print(params, \"TestAccuracy=\", test(model,X_test, Y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selecting Hyperparameters\n",
    "scikit-learn provides a function: GridSearchCV to optimize your neural network's hyper-parameters automatically. We just provide the range or possible value of hyperparameters as the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[CV] activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   20.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=adam -  20.2s\n",
      "[CV] activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=adam -  19.7s\n",
      "[CV] activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=sgd -  17.5s\n",
      "[CV] activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=sgd -  17.8s\n",
      "[CV] activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.001, solver=adam -   4.8s\n",
      "[CV] activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.001, solver=adam -   5.9s\n",
      "[CV] activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.001, solver=sgd -  19.3s\n",
      "[CV] activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=300, learning_rate_init=0.001, solver=sgd -  27.7s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam -  25.2s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam -  25.5s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd -  21.4s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd -  27.3s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam -   6.0s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam -   3.5s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd -  23.8s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd -  20.2s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=adam -  25.5s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=adam \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=adam -  25.6s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=sgd -  21.9s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=sgd -  24.8s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=adam -   3.3s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=adam -   4.2s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=sgd -  22.4s\n",
      "[CV] activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=sgd \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  activation=tanh, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=sgd -  23.7s\n",
      "[CV] activation=relu, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=adam -   4.7s\n",
      "[CV] activation=relu, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=adam -   5.6s\n",
      "[CV] activation=relu, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=sgd -   2.0s\n",
      "[CV] activation=relu, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=300, learning_rate_init=0.0001, solver=sgd -   1.7s\n",
      "[CV] activation=relu, hidden_layer_sizes=300, learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=300, learning_rate_init=0.001, solver=adam -   2.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=300, learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=300, learning_rate_init=0.001, solver=adam -   2.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=300, learning_rate_init=0.001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=300, learning_rate_init=0.001, solver=sgd -   7.5s\n",
      "[CV] activation=relu, hidden_layer_sizes=300, learning_rate_init=0.001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=300, learning_rate_init=0.001, solver=sgd -   7.5s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam -   5.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=adam -   6.0s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd -   2.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.0001, solver=sgd -   2.5s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam -   1.9s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=adam -   2.0s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd -   9.2s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100), learning_rate_init=0.001, solver=sgd -   1.4s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=adam -   7.6s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=adam -   7.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=sgd -   4.7s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.0001, solver=sgd -   7.3s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=adam -   2.4s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=adam \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=adam -   2.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=sgd -   8.1s\n",
      "[CV] activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=sgd \n",
      "[CV]  activation=relu, hidden_layer_sizes=(300, 100, 50), learning_rate_init=0.001, solver=sgd -  15.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'activation': ['tanh', 'relu'], 'learning_rate_init': [0.0001, 0.001], 'hidden_layer_sizes': [300, (300, 100), (300, 100, 50)], 'solver': ['adam', 'sgd']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'activation' : [\"tanh\", \"relu\"],\n",
    "            'learning_rate_init' : [0.0001, 0.001],\n",
    "            'hidden_layer_sizes' : [(300,), (300, 100), (100, 50)],\n",
    "            'solver' : [\"adam\",\"sgd\"]\n",
    "             }\n",
    "clf = MLPClassifier()\n",
    "clf = GridSearchCV(estimator=clf, param_grid=parameters, verbose=2, cv=2)\n",
    "clf.fit(X_train, Y_train)   ## might take about 10 minutes depending on number of total parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=300, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
