{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab10 Experiment 02\n",
    "\n",
    "## Visualizing CNNs ##\n",
    "\n",
    "In this experiment, we will first follow the approach used by [Matthew D Zeiler, Rob Fergus](https://arxiv.org/abs/1311.2901) to visualize CNNs. We will visualize a VGG16 model trained on the Imagenet dataset.\n",
    "\n",
    "We also follow the approach used by [Mahendran and Vedaldi](https://arxiv.org/abs/1412.0035) to understand deep image representations. [Section 6 onwards]\n",
    "\n",
    "## 1. Load the model and see its architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "print(model.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will load all the module details in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(model.features.modules())\n",
    "modules = modules[1:]\n",
    "print(modules,\"\\n\\n\")\n",
    "print(\"third module = \", modules[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and preprocess an image to pass as input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "    image = Variable(preprocess(image).unsqueeze(0))\n",
    "    return image\n",
    "\n",
    "img_raw = Image.open(\"./dog.jpg\")\n",
    "plt.imshow(img_raw)\n",
    "plt.title(\"Image loaded successfully\")\n",
    "\n",
    "img = normalize(img_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize the kernel weights at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(image, layer):\n",
    "    weight_used = []\n",
    "    \n",
    "    ## Gather all Convolution layers and append their corresponding filters in a list\n",
    "    for w in model.features.children():\n",
    "        if isinstance(w, torch.nn.modules.conv.Conv2d):\n",
    "            weight_used.append(w.weight.data)\n",
    "\n",
    "    print(\"(#filters, i/p depth, size of filter) === \",weight_used[layer].shape)\n",
    "    print(\"No. of filters: \", weight_used[layer].shape[0])\n",
    "    filters = []\n",
    "    for i in range(weight_used[layer].shape[0]):\n",
    "        filters.append(weight_used[layer][i,:,:,:].sum(dim=0))    ##summing across input depth(3 in the first layer)\n",
    "        filters[i].div(weight_used[layer].shape[1])\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "    for i in range(int(np.sqrt(weight_used[layer].shape[0])) * int(np.sqrt(weight_used[layer].shape[0]))):\n",
    "        a = fig.add_subplot(np.sqrt(weight_used[layer].shape[0]),np.sqrt(weight_used[layer].shape[0]),i+1)\n",
    "        imgplot = plt.imshow(filters[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "visualize_weights(img, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filters are usually small $(3X3)$ and hence the visualization of filter weights usually doesn't give us a clear understanding of the what the filters learn.\n",
    " \n",
    "Therefore, we will visualize how the the input image looks as it is passed through the various layers in the network.\n",
    "\n",
    "### 4. Visualizing the image as it passes through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(image):\n",
    "    image = torch.sum(image, dim=0)\n",
    "    image = torch.div(image, image.shape[0])\n",
    "    return image\n",
    "\n",
    "def layer_outputs(image):\n",
    "    outputs = []\n",
    "    names = []\n",
    "    \n",
    "    ## feed forward the image through the network and store the outputs\n",
    "    for layer in modules:\n",
    "        image = layer(image) \n",
    "        outputs.append(image)\n",
    "        names.append(str(layer))\n",
    "    \n",
    "    ## for visualization purposes, convert the output into a 2D image by averaging across the filters.\n",
    "    output_im = []\n",
    "    for i in outputs:\n",
    "        i = i.squeeze(0)\n",
    "        temp = to_grayscale(i)  ## convert say 64x112x112 to 112x112\n",
    "        output_im.append(temp.data.numpy())\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (30, 40)\n",
    "\n",
    "\n",
    "    for i in range(len(output_im)):\n",
    "        a = fig.add_subplot(8,4,i+1)\n",
    "        imgplot = plt.imshow(output_im[i])\n",
    "        plt.axis('off')\n",
    "        a.set_title(str(i+1)+\". \"+names[i].partition('(')[0], fontsize=30)\n",
    "\n",
    "#     ##save the resulting visualization\n",
    "#     plt.savefig('layer_outputs.jpg', bbox_inches='tight')\n",
    "\n",
    "##\n",
    "layer_outputs(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the above visualization, it is clearly visible how the CNN responds to an image at each layer and in the final layer, the pixels of the image which produce the highest activation values are visible as well. However, we still haven't looked at how the filters at each layer respond differently to features present in the input.\n",
    "\n",
    "### 5. Visualizing output of each filter at a given layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_outputs(image, layer_to_visualize, num_filters=64):\n",
    "    if layer_to_visualize < 0:\n",
    "        layer_to_visualize += 31\n",
    "    output = None\n",
    "    name = None\n",
    "    \n",
    "    ## get outputs corresponding to the mentioned layer\n",
    "    for count, layer in enumerate(modules):\n",
    "        image = layer(image)\n",
    "        if count == layer_to_visualize: \n",
    "            output = image\n",
    "            name = str(layer)\n",
    "    \n",
    "    filters = []\n",
    "    output = output.data.squeeze()\n",
    "\n",
    "    ## if num_filters==-1, visualize all the filters\n",
    "    num_filters = min(num_filters, output.shape[0])\n",
    "    if num_filters==-1:\n",
    "        num_filters = output.shape[0]\n",
    "\n",
    "    for i in range(num_filters):\n",
    "        filters.append(output[i,:,:])\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "    for i in range(int(np.sqrt(len(filters))) * int(np.sqrt(len(filters)))):\n",
    "        fig.add_subplot(np.sqrt(len(filters)), np.sqrt(len(filters)),i+1)\n",
    "        imgplot = plt.imshow(filters[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "## if num_filters==-1, visualize all the filters\n",
    "filter_outputs(img,0,16)    #visualize the outputs of first 16 filters of the 1st layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above visualization shows that each filter responds differently to an input which implies that each filter learns and extracts different features from an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Understanding Deep Image Representations by Inverting Them [Mahendran, Vedaldi]\n",
    "\n",
    "Like Zeiler and Fergus, their method starts from a specific input image. They record the network’s representation of that specific image and then reconstruct an image that produces a similar code. Thus, their method provides insight into what the activation of a whole layer represent, not what an individual neuron represents.\n",
    "\n",
    "They show what each neuron “wants to see”, and thus what each neuron has learned to look for.\n",
    "\n",
    "To visualize the function of a specific unit in a neural network, we $synthesize$ inputs that cause that unit to have high activation. To synthesize such a “preferred input example”, we start with a random image, meaning we randomly choose a color for each pixel. The image will initially look like colored TV static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_noise_img = Variable(1e-1 * torch.randn(1, 3, 224, 224), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take an image $X$ whose representation $X_0$ at some layer $\"target\\_layer\"$ we want to learn. Our aim is to reconstruct the noise image to get this representation $X_0$. The principle behind this is that the noise image will be so reconstructed such that it will represent what the particular layer for which it is trained against wants to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_at_nth_layer(inp, layer):\n",
    "    for i in range(layer):\n",
    "        inp = modules[i](inp)\n",
    "    return inp[0]\n",
    "\n",
    "## dont forget that the system is 0 indexed\n",
    "target_layer = 18    ## which is this layer Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "inp_img = normalize(Image.open(\"./dog.jpg\"))\n",
    "inp_img_representation = get_output_at_nth_layer(inp_img, target_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions of code (euclidean_loss, alpha_norm, total_variation_norm, recreate_image) are present in utils.py. If you truly want to understand how this is implemented, it is recommended that you readthe second and third page of this [paper](https://arxiv.org/abs/1412.0035), specifically, the regularization part, before asking questions on that. The aim of this code is to mainly understand the deep representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import utils\n",
    "\n",
    "#define optimizers for learning the representation of the noise input image\n",
    "optimizer = SGD([random_noise_img], lr=1e4, momentum=0.9)\n",
    "alpha_reg_alpha = 1\n",
    "alpha_reg_lambda = 1e-7\n",
    "tv_reg_beta = 2\n",
    "tv_reg_lambda = 1e-8\n",
    "## Put model into evaluation state\n",
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=[]\n",
    "for i in range(161):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## get output at the target layer (not the final layer)\n",
    "        output = get_output_at_nth_layer(random_noise_img,target_layer)\n",
    "        \n",
    "        # Calculate euclidian loss between output image and the target image\n",
    "        euc_loss = 1e-1 * utils.euclidian_loss(inp_img_representation.detach(), output)\n",
    "        \n",
    "        # regularization\n",
    "        reg_alpha = alpha_reg_lambda * utils.alpha_norm(random_noise_img, alpha_reg_alpha)\n",
    "        reg_total_variation = tv_reg_lambda * utils.total_variation_norm(random_noise_img,tv_reg_beta)\n",
    "        \n",
    "        loss = euc_loss + reg_alpha + reg_total_variation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Generate image every 5 iterations\n",
    "        if i % 10 == 0:\n",
    "            print('Iteration:', str(i), 'Loss:', loss.data.numpy()[0])\n",
    "            x = utils.recreate_image(random_noise_img)\n",
    "            imgs.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the learned representations for the layer \"target_layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "for i in range(int(np.sqrt(len(imgs))) * int(np.sqrt(len(imgs)))):\n",
    "    a = fig.add_subplot(np.sqrt(len(imgs)), np.sqrt(len(imgs)),i+1)\n",
    "    imgplot = plt.imshow(imgs[i])\n",
    "    a.set_title(\"iter = \"+ str((i+1)*10))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Above we visualized the inverted representations for the 18th layer, find similar representations for different layers in the network and thus visualize what the network learns at each of those layer. Try for different target images as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
