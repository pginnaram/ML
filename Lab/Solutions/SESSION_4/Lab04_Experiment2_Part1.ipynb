{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab04 Experiment 02 - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "In this experiment, we will learn to build a machine learning pipeline step by step. As mentioned\n",
    "in the last lecture, a machine learning system consists of the following depending on whether the\n",
    "system is in training or testing phase:\n",
    "\n",
    "### **Training**:\n",
    "\n",
    "- **Getting Data** - While solving real-world problems in the “wild”, data is seldom present in pretty formats like .csv. Parsing the data is an integral part of machine learning in which data science engineers spend majority of their time.\n",
    "\n",
    "- **Feature Extraction** - This step involves cleaning the data, identifying the useful attributes/features for solving the problem, interpolating values of missing attributes and reducing the dimensionality of the data if required.\n",
    "\n",
    "- Deciding on a learning algorithm and model hyperparameters.\n",
    "\n",
    "- **Validation** - Validating the generated model on a validation set with a suitable evaluation metric.\n",
    "\n",
    "The model and parameters which result in highest validation accuracy are used in the\n",
    "testing phase.\n",
    "\n",
    "### **Testing**:\n",
    "\n",
    "- **Getting Data** - Getting data which is unseen by the model till now.\n",
    "\n",
    "- **Feature Extraction** - This step is carried out in the same manner as during training phase. The same set of features and processing pipeline are used during training and testing.\n",
    "\n",
    "- **Prediction** - Prediction using the model obtained in the training phase. No part of the model state (including the hyperparameters) can be tuned at this point\n",
    "\n",
    "Let us go through these step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !apt-get update\n",
    "## !apt-get -y install python-opencv\n",
    "#!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING THE DATA\n",
    "\n",
    "The dataset that we shall be using is a subset of the [VGG-Face](http://www.robots.ox.ac.uk/~vgg/data/vgg_face/) dataset. It consists of 20 images each of 10 Indian celebrities. We have already pre-processed it and it is availabale in a directory called vgg_face_indian_dataset, inside the Datasets folder.\n",
    "\n",
    "**DO NOT download anyfiles from the link above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indians = [\"A.R._Rahman\", \"Aamir_Khan\", \"Amitabh_Bachchan\", \"A_P_J_Abdul_Kalam\", \"Kamal_Hassan\", \"Madhuri_Dixit\", \"Mahendra_Singh_Dhoni\", \"Preity_Zinta\", \"Vidya_Balan\", \"Virat_Kohli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see a raw image in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory names\n",
    "dataset_dir = '../Datasets/vgg_face_indian_dataset'\n",
    "raw_images_dir = '../Datasets/vgg_face_indian_dataset/raw'\n",
    "face_images_dir = '../Datasets/vgg_face_indian_dataset/faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image in the \"raw\" directory\n",
    "example_raw_image = cv2.imread(os.path.join(raw_images_dir, \"A.R._Rahman_01.jpg\"))\n",
    "# Show it\n",
    "plt.imshow(cv2.cvtColor(example_raw_image, cv2.COLOR_BGR2RGB)) # Ignore \"cv2.COLOR_BGR2RGB\"\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, 20 images each of 10 celebrities.\n",
    "\n",
    "The bounding box of the faces in these image have been found, and the images have been rotated to make the faces straight, and then converted to grayscale. These grayscale images have been saved in the **face_images_dir** directory.\n",
    "\n",
    "Let us read all the images in the faces dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all images in the \"faces\" directory\n",
    "images = []\n",
    "for indian in indians:\n",
    "    images.append([])\n",
    "    # The code inside the loop constructs the filename as Name_nn.jpg for each name\n",
    "    # nn running from 01 to 20\n",
    "    for i in range(1, 21):\n",
    "        filenum = \"_{0:02d}\".format(i)\n",
    "        filename = face_images_dir + \"/\" + indian + filenum + \".jpg\"\n",
    "        images[-1].append(cv2.imread(filename, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$images$ is a list containing 10 lists. Each of those 10 lists contains images of a celebrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the length of (or, number of elements in) \"images\", and the type of data each of those elements is\n",
    "print(len(images), type(images[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 10 lists contains 20 images, i.e. 20 images per celebrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the length of each list in \"images\"\n",
    "print(len(images[0]), type(images[0][0]))\n",
    "print(len(images[1]), type(images[1][0]))\n",
    "print(len(images[2]), type(images[2][0]))\n",
    "print(len(images[3]), type(images[3][0]))\n",
    "print(len(images[4]), type(images[4][0]))\n",
    "print(len(images[5]), type(images[5][0]))\n",
    "print(len(images[6]), type(images[6][0]))\n",
    "print(len(images[7]), type(images[7][0]))\n",
    "print(len(images[8]), type(images[8][0]))\n",
    "print(len(images[9]), type(images[9][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these 20 images per celebrity is a \"numpy array\". These images are of variable size. For example, the first two images of A.R. Rahman are of sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_rahman_index = 0\n",
    "print(images[ar_rahman_index][0].shape, images[ar_rahman_index][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, all the images are of different sizes. Let us visualize one image per class (celebrity) in the dataset, and note the sizes of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one image of each class\n",
    "\n",
    "# Number of images ot be plotted\n",
    "N = 10\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# For each class\n",
    "for i, indian in enumerate(indians):\n",
    "    \n",
    "    # Make subplot\n",
    "    # Syntax - plt.subplot(number_of_rows, number_of_columns, current_image_number)\n",
    "    # The 'current_image_number' starts from 1\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    \n",
    "    # Plot the 0th image\n",
    "    example_face_image = images[i][0]\n",
    "    plt.imshow(example_face_image, cmap=\"gray\")\n",
    "    \n",
    "    # Turn off axis lines\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # (Optional) Write the size of the image as its title\n",
    "    plt.title(\"size=\"+str(example_face_image.shape), size=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Splitting the dataset into \"train\", \"val\" and \"test\"\n",
    "\n",
    "As mentioned in the lesson, usually datasets are split into 3 parts - **train**, **validation**, and **test**.\n",
    "\n",
    "Earlier, we used to split datasets into training and testing only - training has ground truth labels but testing does not (in real cases). But it is important to understand how a model will perform on unseen data.\n",
    "\n",
    "For this purpose, the training data is further split into training and validation data. A model is trained on the training data, and predictions are made on the validation data. Then, since the ground truth of the validation data is available, the model's accuracy on the validation data is calculated. This accuracy gives a good insight into how well the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring class balance\n",
    "\n",
    "It is important to maintain equal number of images per celebrity in the train, val and test datasets. This ensures that the model and the calculated accuracies are not biased towards any class. We know that we have 20 images per celebrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1: Edit 3 lines of code to split the \"images\", such that 60% is assigned as training data, 20% as validation, and 20% as test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images in training set per celebrity\n",
    "number_of_images_per_class_in_train = 12\n",
    "\n",
    "# Number of images in validation set per celebrity\n",
    "number_of_images_per_class_in_val = 4\n",
    "\n",
    "# Number of images in test set per celebrity\n",
    "number_of_images_per_class_in_test = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these values, let us make the train, val and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the train, val, and test sets\n",
    "images_train = []\n",
    "images_val = []\n",
    "images_test = []\n",
    "\n",
    "# For each celebrity\n",
    "for i, indian in enumerate(indians):\n",
    "    \n",
    "    # Add a new empty list item\n",
    "    images_train.append([])\n",
    "    images_val.append([])\n",
    "    images_test.append([])\n",
    "    \n",
    "    # Add the specified number of images to the train set\n",
    "    for train_iter in range(0, int(number_of_images_per_class_in_train)):\n",
    "        images_train[-1].append(images[i][train_iter])\n",
    "    \n",
    "    # Add the specified number of images to the val set\n",
    "    for val_iter in range(int(number_of_images_per_class_in_train),\n",
    "                          int(number_of_images_per_class_in_train + number_of_images_per_class_in_val)):\n",
    "        images_val[-1].append(images[i][val_iter])\n",
    "    \n",
    "    # Add the specified number of images to the test set\n",
    "    for test_iter in range(int(number_of_images_per_class_in_train + number_of_images_per_class_in_val),\n",
    "                           int(number_of_images_per_class_in_train + number_of_images_per_class_in_val + number_of_images_per_class_in_test)):\n",
    "        images_test[-1].append(images[i][test_iter])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE MANIPULATIONS\n",
    "\n",
    "In order to train a model, the images have to be manipulated so as to have similar properties. We shall see these manipulation tasks below.\n",
    "\n",
    "We shall be manipulating on the images of the training data first, step-by-step. Then, we shall make a function of all the manipulation tasks, and apply them on validation and testing images as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the images are of different sizes. The A.R.Rahman image is of size (164, 165), i.e. 164 rows and 165 columns, while the Amitabh Bachchan image is of size (206, 206).\n",
    "\n",
    "We need to resize all the images to a constant size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2: Write 1 line to the code below to resize the images to (224, 224)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images\n",
    "resized_images_train = np.zeros((10, int(number_of_images_per_class_in_train), 224, 224)) # 10 celebrities, 12 images per celebrity in \"train\", each image of size (224, 224)\n",
    "for i in range(len(indians)):\n",
    "    for j in range(int(number_of_images_per_class_in_train)):\n",
    "        image = images_train[i][j]\n",
    "        \n",
    "        # Code to resize image\n",
    "        # (Hint: look for \"python3 cv2 resize\")\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "        \n",
    "        resized_images_train[i][j] = resized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot one **resized_image** of each class to check if resizing worked (using the same code as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images to be plotted\n",
    "N = 10\n",
    "\n",
    "# Plot one image of each class\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# For each class\n",
    "for i, indian in enumerate(indians):\n",
    "    \n",
    "    # Make a subplot\n",
    "    # Syntax - plt.subplot(number_of_rows, number_of_columns, current_image_number)\n",
    "    # The 'current_image_number' starts from 1\n",
    "    plt.subplot(N//5, 5, i+1)\n",
    "    \n",
    "    # Plot the image\n",
    "    example_face_image = resized_images_train[i][0]  # Changed \"images_train\" to \"resized_images_train\"\n",
    "    plt.imshow(example_face_image, cmap=\"gray\")\n",
    "    \n",
    "    # Turn off axis lines\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # (Optional) Write the size of the image as a title\n",
    "    plt.title(\"size=\"+str(example_face_image.shape), size=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can (hopefully) see that all the images are now $224\\times224$, so resizing worked!\n",
    "\n",
    "---- **(If this is not the case, please recheck your code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Normalization\n",
    "\n",
    "As an example, let us see the pixel values of the 4th image of Madhuri Dixit. We know that image pixel values range between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "madhuri_dixit_index = 5\n",
    "example_image = resized_images_train[madhuri_dixit_index][4]\n",
    "# When displaying image using plt.imshow, we specify that the\n",
    "# minimum and maximum possible values of the image are 0 and 255\n",
    "# in the arguments vmin and vmax\n",
    "plt.imshow(example_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the minimum and maximum values of this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the minimum and maximum values of the image\n",
    "print(resized_images_train[madhuri_dixit_index][4].min(), resized_images_train[madhuri_dixit_index][4].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the maximum pixel value in this image is 171, while we know that the maximum possible pixel value of an image is 255. No wonder this image appears dark.\n",
    "\n",
    "Let us scale the pixels values in each image in the dataset such that the minimum pixel value within the image becomes 0, and the maximum becomes 255. This way, we are ensuring that the full range of values available (0-255) are being covered in the image. This should result in the above image becoming _brighter_.\n",
    "\n",
    "From the previous lessons, we know that this is called **Min-Max Scaling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: Write 1 line of code below to normalize the images using Min-Max Scaling, such that the minimum and maximum pixel values in each image are 0 and 255**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Scaling of images\n",
    "minmax_scaled_images_train = np.zeros((10, int(number_of_images_per_class_in_train), 224, 224)) # 10 celebrities, some images per celebrity, each image of size (224, 224)\n",
    "for i in range(len(indians)):\n",
    "    for j in range(int(number_of_images_per_class_in_train)):\n",
    "        resized_image = resized_images_train[i][j]\n",
    "        \n",
    "        # Code to normalize image using minmax scaling\n",
    "        # (Hint: Look in numpy's documentation for minimum and maximum)\n",
    "        minmax_scaled_image = (resized_image - np.min(resized_image))/(np.max(resized_image) - np.min(resized_image))*255\n",
    "        \n",
    "        minmax_scaled_images_train[i][j] = minmax_scaled_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the 4th Madhuri Dixit image before and after min-max scaling to see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the minimum and maximum values of the image\n",
    "print(minmax_scaled_images_train[madhuri_dixit_index][4].min(), minmax_scaled_images_train[madhuri_dixit_index][4].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing image before and after min-max scaling\n",
    "madhuri_dixit_index = 5\n",
    "plt.subplot(121)\n",
    "plt.imshow(resized_images_train[madhuri_dixit_index][4], cmap='gray', vmin=0, vmax=255)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(minmax_scaled_images_train[madhuri_dixit_index][4], cmap='gray', vmin=0, vmax=255)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can (hopefully) see that the image does become brighter, and its minimum and maximum values are 0 and 255. Min-max scaling worked!\n",
    "\n",
    "---- **(If this not the case, please recheck your code.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combining 1st and 2nd dimensions\n",
    "\n",
    "Till now, we maintained the 1st dimension as iterating through celebrities, and the 2nd dimension as iterating throught the images of each celebrity. This was done only for illustrative purposes. $minmax\\_scaled\\_images\\_train$ is of shape $10\\times num\\_of\\_images\\_per_\\_class\\_in\\_train\\times224\\times224$.\n",
    "\n",
    "But, usually datasets have the first dimension as iterating through all the samples, without further division. So let us combine the first and second dimensions to make the new shape $(10 * num\\_of\\_images\\_per\\_class\\_in\\_train)\\times224\\times224$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining 1st and 2nd dimensions\n",
    "collated_images = np.reshape(minmax_scaled_images_train, (10*int(number_of_images_per_class_in_train), 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reshaping to $n\\times K^2$\n",
    "\n",
    "If $n$ is the number of samples in a set of images, and $K$ is the number or rows (or columns) in an image, we would like our dataset to be in $n\\times K^2$ shape, so samples are the first dimension, and features (all the pixel values) are the second dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4: Write 1 line of code to _reshape_ the data into an $n\\times K^2$ shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping into n x K^2\n",
    "reshaped_images = np.reshape(collated_images, (collated_images.shape[0], 224*224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subtracting $mean\\_image$\n",
    "\n",
    "Since we are interested in the difference between the faces, let’s subtract the characteristics which are common between them. The common characteristic of each pixel value is its mean among all the training images. Thus, let us find the $mean\\_image$, and subtract it from all the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5: Write 1 line of code to find the mean image of the training images, using numpy**. Hint: Check for the \"axis\" argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean image\n",
    "mean_image = np.mean(reshaped_images, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your mean image is of the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- **If the image shape is not (50176,), i.e. 224^2, please recheck your code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now subtract the $mean\\_image$ from all the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting mean image from the training images\n",
    "final_images_train = reshaped_images - mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing mean image\n",
    "\n",
    "For curiosity's sake, let us see how the mean image looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(mean_image, (224, 224)), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the mean image consists of a pair of eyes, a nose, a mouth, etc., at the right places. This is possible since all the face images in the dataset were oriented and aligned to each other.\n",
    "\n",
    "Had the images not been aligned, the mean image would have looked like this:\n",
    "\n",
    "<img src=\"../Datasets/vgg_face_indian_dataset/hazy_mean_image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION AND TEST DATA\n",
    "\n",
    "## - Image manipulations for validation and test data\n",
    "\n",
    "The same image manipulations must be carried out on the validation and test images. Below is a function that performs all the operations step-by-step. Note that the mean image is calculated only on the training set.\n",
    "\n",
    "**Please copy-paste the solutions to the relevant exercises (2, 3, and 4) at the respective places**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_images(images, number_of_images_per_class, mean_image):\n",
    "    \n",
    "    # Manipulated images\n",
    "    manipulated_images = np.zeros((10, int(number_of_images_per_class), 224, 224)) # 10 celebrities, some images per celebrity, each image of size (224, 224)\n",
    "    \n",
    "    for i in range(len(indians)):\n",
    "        for j in range(int(number_of_images_per_class)):\n",
    "            image = images[i][j]\n",
    "\n",
    "            # Code to resize image\n",
    "            # (Hint: look for \"python3 cv2 resize\")\n",
    "            resized_image = cv2.resize(image, (224, 224))\n",
    "            \n",
    "            # Code to normalize image using minmax scaling\n",
    "            # (Hint: Look in numpy's documentation for minimum and maximum)\n",
    "            minmax_scaled_image = (resized_image - np.min(resized_image))/(np.max(resized_image) - np.min(resized_image))*255\n",
    "            \n",
    "            manipulated_images[i][j] = minmax_scaled_image\n",
    "            \n",
    "    # Combining 1st and 2nd dimensions\n",
    "    collated_images = np.reshape(manipulated_images, (10*number_of_images_per_class, 224, 224))\n",
    "    \n",
    "    # Reshaping into n x K^2\n",
    "    reshaped_images = np.reshape(collated_images, (collated_images.shape[0], 224*224))\n",
    "    \n",
    "    # Subtracting mean image from the training images\n",
    "    final_images = reshaped_images - mean_image\n",
    "    \n",
    "    return final_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this function to find the final images of val and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding final images for val and test\n",
    "final_images_val = manipulate_images(images_val, number_of_images_per_class_in_val, mean_image)\n",
    "final_images_test = manipulate_images(images_test, number_of_images_per_class_in_test, mean_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABELS\n",
    "\n",
    "Let us make the labels for train, val and test. Each of the 10 celebrities shall be associated with a number among {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = np.array([[i]*number_of_images_per_class_in_train for i in range(10)]).flatten()\n",
    "labels_val = np.array([[i]*number_of_images_per_class_in_val for i in range(10)]).flatten()\n",
    "labels_test = np.array([[i]*number_of_images_per_class_in_test for i in range(10)]).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we know that there are 4 images per celebrity in the validation set of 40 images. Their labels now are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE DATA\n",
    "\n",
    "Let us save the data as \"data.npz\" format, so that we can load it in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train, val, and test data and labels, and the mean image\n",
    "np.savez(\"data\",\n",
    "         data_train=final_images_train, labels_train=labels_train,\n",
    "         data_val=final_images_val, labels_val=labels_val,\n",
    "         data_test=final_images_test, labels_test=labels_test,\n",
    "         mean_image=mean_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
