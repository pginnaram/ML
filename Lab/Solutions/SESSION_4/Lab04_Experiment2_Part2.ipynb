{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab04 Experiment 02 - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "In this experiment, we shall see:\n",
    "\n",
    "- **1. FEATURE EXTRACTION**\n",
    "    - extract eigenvalues and eigenvectors, and choose the best N principal components as features\n",
    "    - use deep features\n",
    "\n",
    "\n",
    "- **2. MODEL SELECTION**: try 6 different models, and choose the model with the highest validation accuracy:\n",
    "\n",
    "    - 1) use Naive Bayes classifier with PCA features, and find validation accuracy\n",
    "    - 2) use Naive Bayes classifier with Deep features, and find validation accuracy\n",
    "    - 3) use kNN classifier with PCA features, and find validation accuracy\n",
    "    - 4) use kNN classifier with Deep features, and find validation accuracy\n",
    "    - 5) use linear classifier with PCA features, and find validation accuracy\n",
    "    - 6) use linear classifier with Deep features, and find validation accuracy\n",
    "\n",
    "\n",
    "- **3. PREDICTION**: use the 6 models to predict labels on test data, and see how well the validation accuracy estimated the test accuracy.\n",
    "\n",
    "\n",
    "- **4. COMPARISON OF ALL ACCURACIES**: compare training, validation and testing accuracies.\n",
    "\n",
    "Let us go through these step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from classifiers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap of Part 1\n",
    "\n",
    "In Part 1, we understood the data, and split it into train, val, and test. We then manipulated it so that the data is of uniform size, normalized, and mean subtracted. (Not that these operations can be performed on any data, not just images.)\n",
    "\n",
    "We then saved the final datasets as \"data.npz\" file. Let us load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = np.load(\"data.npz\")\n",
    "\n",
    "# Getting train, val and test data and labels, and the mean_image\n",
    "data_train = data[\"data_train\"]\n",
    "labels_train = data[\"labels_train\"]\n",
    "data_val = data[\"data_val\"]\n",
    "labels_val = data[\"labels_val\"]\n",
    "data_test = data[\"data_test\"]\n",
    "labels_test = data[\"labels_test\"]\n",
    "mean_image = data[\"mean_image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6: Plot the first 50 training images. You can use _matplotlib_ or any library of your choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the first 50 images\n",
    "# Your code here\n",
    "N = 50\n",
    "plt.figure(figsize=(10, 2*(N+5)//5))\n",
    "\n",
    "for i in range(0,51):\n",
    "    plt.subplot((N+5)//5, 5, i+1)\n",
    "    \n",
    "    image = np.reshape(data_train[i],(224,224))\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    \n",
    "    # Turn off axis lines\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # (Optional) Write the size of the image as a title\n",
    "    plt.title(\"size=\"+str(image.shape), size=18)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given problem, we would be using **two** types of features - PCA, and Deep Features.\n",
    "\n",
    "After this exercise, we will compare the results using the two different types of features on the validation set. We shall then fix the final model based on which feature set performs better according to an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. First $N$ Principal Components with maximum eigenvalues\n",
    "\n",
    "As we have learnt in the lecture, PCA finds the set of orthonormal vectors which best\n",
    "describe the distribution of the underlying dataset. In the given dataset, we have $n$\n",
    "images of size $K \\times K$. (We know that $K = 224$, and $n = 120$ in the training set)\n",
    "\n",
    "We have already seen how PCA features are extracted, let us recap them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1. Equations related to eigenvalues and eigenvectors\n",
    "\n",
    "The first step towards finding Principal Components is to find the eigenvalues and eigenvectors of the co-variance matrix of our data.\n",
    "\n",
    "From the last subsection, we have the data matrix $\\pmb A$ as an $n \\times K^2$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning data_train to \"A\"\n",
    "A = data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pmb X$, the covariance matrix of $\\pmb A$, is shown in the below equation:\n",
    "\n",
    "$$\n",
    "% X = \\frac{1}{n}\\sum_{i=1}^{n}A^T.A\n",
    "\\pmb X = \\frac{1}{n}\\ (\\pmb A^T.\\pmb A)\n",
    "$$\n",
    "\n",
    "This can be coded in Python as : `X = 1 / A.shape[0] * np.dot(A.T, A)`. Its size is $K^2 \\times K^2$, i.e. the shape of $\\pmb X$ is $(K^2, K^2)$.\n",
    "\n",
    "The eigenvaues and eigenvectors of $\\pmb X$ can be found in Python as: `w, v = np.linalg.eig(X)`, where `w` are the eigenvalues, and `v` is the matrix of eigenvectors.\n",
    "\n",
    "Since $\\pmb X$ is a $K^2 \\times K^2$ matrix, there should be $K^2$ eigenvalues, and $K^2$ eigenvectors each of $K^2$ dimensions. So `w` is a numpy array of shape ($K^2$,), i.e. it contains $K^2$ number of eigenvalues. `v` is a numpy array of shape ($K^2$, $K^2$), where each **column** of `v` is an eigenvector. So there are $K^2$ eigenvectors, each of shape ($K^2$,)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2. Find eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7: Use the solution to Excercise 7, and write a function to find the eigenvalues and eigenvectors of the covariance of matrix A.**\n",
    "\n",
    "The input to this function is a data matrix $\\pmb A$ (i.e. `data_train`), and the outputs are `w` and `v`, the eigenvalues and eigenvectors of $\\pmb A^T.\\pmb A$.\n",
    "\n",
    "Hint: Use `np.linalg.eig()` function to find the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eigenvalues_and_eigenvectors_simply(A):\n",
    "    # Your code here\n",
    "    L = 1 / len(A.T) * np.dot(A, A.T)\n",
    "    e, u = np.linalg.eig(L)\n",
    "    w = e\n",
    "    v = np.dot(A.T, u)\n",
    "    return w, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above function, let us find the eigenvalues and eigenvectors in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = find_eigenvalues_and_eigenvectors_simply(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3. Reordering, normalizing\n",
    "\n",
    "But, since we found the eigenvalues and eigenvectors in a roundabout way, we need to:\n",
    "- reorder them so that they are in descending order of eigenvalues,\n",
    "- normalize the eigenvectors so that their norms are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REORDER\n",
    "\n",
    "# Find the required order of indices to make decreasing order of eigenvalue\n",
    "sort_index = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "# Use the calculated order of indices to reorder eigenvalues and eigenvectors\n",
    "eigenvalues = eigenvalues[sort_index]\n",
    "eigenvectors = eigenvectors[:, sort_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE\n",
    "eigenvectors = eigenvectors / np.linalg.norm(eigenvectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4. Eigenfaces\n",
    "\n",
    "The eigenvectors thus found are called eigenfaces (because we found the eigenvectors of faces...).\n",
    "\n",
    "Since an eigenvector is of dimension ($K^2$,), it can be reshaped to $(K, K)$ and displayed as an image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8: Display the first 50 eigenfaces.** Hint: we solved a similar problem in Exercise 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the first 50 eigenfaces\n",
    "# Your code here\n",
    "N = 50\n",
    "\n",
    "plt.figure(figsize=(10, 2*(N+5)//5))\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    # Make a subplot\n",
    "    plt.subplot((N + 5)//5, 5, i+1)\n",
    "    \n",
    "    # Plot the eigenface, after reshaping it to (224, 224)\n",
    "    # Remember eigenfaces are **columns** in the matrix\n",
    "    plt.imshow(np.reshape(eigenvectors[:, i], (224, 224)), cmap='gray')\n",
    "    \n",
    "    # Turn off axis lines\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.5. Computing good value for $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(already seen in previous lab)_\n",
    "\n",
    "In the given dataset, there are as many eigenvectors as the number of training examples. This can be verified by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each column is an eigenvector, there are 120 eigenvectors, each of 50176 dimensions. But usually, a smaller number $N$ of eigenvectors is chosen as a basis to make feature vectors.\n",
    "\n",
    "To decide the on the number $N$, i.e. the number of most important eigenvectors to keep as the basis, the cumulative sum of eigenvalues (assuming they are in decreasing order) divided by the total sum of eigenvalues, vs. the number of eigenvalues considered ($M$) is plotted.\n",
    "\n",
    "This plot shall show the fraction of total variance retained ($r$) vs. the number of eigenvalues considered ($M$). This way, the plot gives a good understanding of the point of diminishing returns, i.e. the point where little variance is retained by retaining additional eigenvalues.\n",
    "\n",
    "This can be understood by the following equation:\n",
    "\n",
    "$$r = \\frac{\\sum_{k=1}^{M}\\lambda_k}{\\sum_{k=1}^{n}\\lambda_k},\\ \\ \\ \\  M <= n$$\n",
    "\n",
    "Plotting $r$ vs $M$ shall give a good idea of the impact of varying $M$ on $r$.\n",
    "\n",
    "Let's say we want to retain only 80% of the variance involved. Then we should look for the minimum value of $M$ for which $r > 0.8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot $r$ vs $M$:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot eigenvalues vs M\n",
    "\n",
    "# Values of M to consider: 1, 2,..., n\n",
    "M = np.array(range(1, len(data_train) + 1))\n",
    "\n",
    "# Plot eigenvalues vs M\n",
    "plt.plot(M, eigenvalues)\n",
    "plt.xlabel(\"M\", fontsize=20)\n",
    "plt.ylabel(\"eigenvalues\", fontsize=20)\n",
    "plt.grid(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot that an $M$ value of around 20 gives an $r$ value of 0.8.\n",
    "\n",
    "So let us choose $N = 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we are choosing only $N$ **principal components**. In other words, we are choosing those $N$ types of information that are most important in faces. We can look at the plots of the eigenfaces to see what sort of information we are choosing.\n",
    "\n",
    "Let us note the first N principal components, i.e. the first N eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vectors = eigenvectors[:, :N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.6. Finding features using first $N$ Principal Components\n",
    "\n",
    "_(already seen in previous lab)_\n",
    "\n",
    "Since we are using the most important eigenfaces as the _basis_ vectors, we need to project the data into these basis components to find the relevant features. We do this by finding the dot product of the data maxtrix and the matrix of the most important eigenvectors.\n",
    "\n",
    "We know that the data (`data_train`, `data_val`, `data_test`) is of shape $n \\times K^2$. We also know that the `pca_vectors` matrix is of shape $K^2 \\times N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features_train = np.dot(data_train, pca_vectors)\n",
    "pca_features_val = np.dot(data_val, pca_vectors)\n",
    "pca_features_test = np.dot(data_test, pca_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shapes of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca_features_train.shape, pca_features_val.shape, pca_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can see that we have transformed our data from $n \\times K^2$ to $n \\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Deep features\n",
    "\n",
    "We used a deep neural network to transform the images into 2622 good features. Let us read these features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep features\n",
    "deep_features = np.load(\"../Datasets/vgg_face_indian_dataset/vgg_face_deep_features.npz\")\n",
    "deep_features_train = deep_features[\"train\"]\n",
    "deep_features_val = deep_features[\"val\"]\n",
    "deep_features_test = deep_features[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deep_features_train.shape, deep_features_val.shape, deep_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training and Prediction\n",
    "\n",
    "Till now, we have gotten familiar with kNN algorithm and linear classifier. We have PCA features and Deep features at our disposal.\n",
    "\n",
    "Let us now train the following 6 models on the training set, and compare accuracies on the test set to see which model performs better using which feature:\n",
    "\n",
    "- 1) Naive bayes classifer on PCA features,\n",
    "- 2) Naive bayes classifer on deep features,\n",
    "- 3) kNN classifier on PCA features,\n",
    "- 4) kNN classifier on Deep features,\n",
    "- 5) linear classifier on PCA features,\n",
    "- 6) linear classifier on Deep features,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to choose algorithm, features and labels to get accuracy\n",
    "\n",
    "We created a convenient function to which you can choose the algorithm you want to use - `'kNN'`, `'NB'` (Naive Bayes), or `'linear'` (multiclass linear classifier) - and input features and labels for training and testing:\n",
    "\n",
    "**`acc = predict_and_find_accuracy(algorithm, train_features, train_labels, given_features, given_labels, k)`**\n",
    "\n",
    "INPUTS:\n",
    "\n",
    "- **`algorithm`** is one of `'kNN'`, `'NB'` (Naive Bayes), and `'linear'`,\n",
    "\n",
    "\n",
    "- **`train_features`** are either `pca_features_train` or `deep_features_train`,\n",
    "\n",
    "\n",
    "- **`train_labels`** are `labels_train`,\n",
    "\n",
    "\n",
    "- **`given_features`** are `XXX_features_val` in case of validation, and `XXX_features_test` in case of testing (`XXX` being either `pca` or `deep` depending on what you chose to train the model on)\n",
    "\n",
    "\n",
    "- **`given_labels`** are `labels_val` in case of validation, and `labels_test` in case of testing.\n",
    "\n",
    "\n",
    "- **`k`** is the value of $k$ to be used _in case_ we are using kNN classifier.\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "- **acc** is the accuracy of the `algorithm` on `given_features` & `given_labels` after being trained on `train_features` & `train_labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Test accuracy with PCA features\n",
    "\n",
    "**Exercise 9: Find the test accuracy of a Naive Bayes classifier on PCA features using the convenient function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pca_test_acc = predict_and_find_accuracy(\"NB\", pca_features_train, labels_train, pca_features_test, labels_test)\n",
    "print(NB_pca_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Test accuracy with Deep features\n",
    "\n",
    "**Exercise 10: Find the test accuracy of a Naive Bayes classifier on deep features using the convenient function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_deep_test_acc = predict_and_find_accuracy(\"NB\", deep_features_train, labels_train, deep_features_test, labels_test)\n",
    "print(NB_deep_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. k Nearest Neighbours\n",
    "\n",
    "By now, we are quite familiar with the kNN algorithm.\n",
    "\n",
    "But how to find the best value of $k$? We will use the validation set to determine this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Find best $k$ for PCA features\n",
    "\n",
    "**Exercise 11:** Using the convenient function, find the validation accuracies of a kNN classifier on pca_features using different values of $k$. Then, choose that $k$ which gives the best validation accuracy.\n",
    "\n",
    "We shall choose a $k$ among {1, 3, 5, 7, 9, 11}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_of_k = [1, 3, 5, 7, 9, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "best = 0\n",
    "result = 0\n",
    "for k in values_of_k:\n",
    "    kNN_test_acc = predict_and_find_accuracy(\"kNN\", pca_features_train, labels_train, pca_features_test, labels_test,k)\n",
    "    print(k, kNN_test_acc)\n",
    "    if best < kNN_test_acc:\n",
    "        best = kNN_test_acc\n",
    "        result = k\n",
    "\n",
    "best_pca_k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_pca_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best $k$ for PCA should be 1.\n",
    "\n",
    "---- **(If not, please recheck your code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 Find best $k$ for Deep features\n",
    "\n",
    "**Exercise 12:** Using the convenient function, find the validation accuracies of a kNN classifier on deep_features using different values of $k$. Then, choose that $k$ which gives the best validation accuracy.\n",
    "\n",
    "We shall choose a $k$ among {1, 3, 5, 7, 9, 11}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "best = 0\n",
    "result = 0\n",
    "for k in values_of_k:\n",
    "    kNN_test_acc = predict_and_find_accuracy(\"kNN\", deep_features_train, labels_train, deep_features_test, labels_test,k)\n",
    "    print(k, kNN_test_acc)\n",
    "    if best < kNN_test_acc:\n",
    "        best = kNN_test_acc\n",
    "        result = k\n",
    "        \n",
    "best_deep_k = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_deep_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best $k$ for deep features should be 1.\n",
    "\n",
    "---- **(If not, please recheck your code)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3 Test accuracy with PCA features\n",
    "\n",
    "**Exercise 13**: Using the convenient function, find the test accuracy of a kNN classifier with the best value of $k$ on pca_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN_pca_test_acc = predict_and_find_accuracy(\"kNN\", pca_features_train, labels_train, pca_features_test, labels_test,best_pca_k)\n",
    "print(kNN_pca_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4 Test accuracy with Deep features\n",
    "\n",
    "**Exercise 14**: Using the convenient function, find the test accuracy of a kNN classifier with the best value of $k$ on deep_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN_deep_test_acc = predict_and_find_accuracy(\"kNN\", deep_features_train, labels_train, deep_features_test, labels_test,best_deep_k)\n",
    "print(kNN_deep_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Multi-class Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Test accuracy with PCA features\n",
    "\n",
    "**Exercise 15: Find the test accuracy of a linear classifier on PCA features using the convenient function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pca_test_acc = predict_and_find_accuracy(\"linear\", pca_features_train, labels_train, pca_features_train, labels_train)\n",
    "print(linear_pca_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Test accuracy with Deep features\n",
    "\n",
    "**Exercise 16: Find the test accuracy of a linear classifier on Deep features using the convenient function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_deep_test_acc = predict_and_find_accuracy(\"linear\", deep_features_train, labels_train, deep_features_train, labels_train)\n",
    "print(linear_deep_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 17: (Discuss) If this were a _retrieval_ problem, what other metrics would you evaluate to compare different models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer :: precision@K and recall@K are more relevant metrics for retrieval problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. COMPARISON OF ACCURACIES OVER THE 6 MODELS\n",
    "\n",
    "We have trained and tested 6 different models:\n",
    "- 1) Naive Bayes classifier on PCA features,\n",
    "- 2) Naive Bayes classifier on Deep features,\n",
    "- 3) kNN classifier on PCA features,\n",
    "- 4) kNN classifier on Deep features,\n",
    "- 5) linear classifier on PCA features,\n",
    "- 6) linear classifier on Deep features,\n",
    "\n",
    "As an additional note, we shall find the training and validation accuracies for each model, and see all the accuracies side-by-side: training accuracy, validation accuracy and test accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding training accuracies\n",
    "NB_pca_train_acc = predict_and_find_accuracy(\"NB\", pca_features_train, labels_train, pca_features_train, labels_train)\n",
    "NB_deep_train_acc = predict_and_find_accuracy(\"NB\", deep_features_train, labels_train, deep_features_train, labels_train)\n",
    "kNN_pca_train_acc = predict_and_find_accuracy(\"kNN\", pca_features_train, labels_train, pca_features_train, labels_train, best_pca_k)\n",
    "kNN_deep_train_acc = predict_and_find_accuracy(\"kNN\", deep_features_train, labels_train, deep_features_train, labels_train, best_deep_k)\n",
    "linear_pca_train_acc = predict_and_find_accuracy(\"linear\", pca_features_train, labels_train, pca_features_train, labels_train)\n",
    "linear_deep_train_acc = predict_and_find_accuracy(\"linear\", deep_features_train, labels_train, deep_features_train, labels_train)\n",
    "\n",
    "# Finding validation accuracies\n",
    "NB_pca_val_acc = predict_and_find_accuracy(\"NB\", pca_features_train, labels_train, pca_features_val, labels_val)\n",
    "NB_deep_val_acc = predict_and_find_accuracy(\"NB\", deep_features_train, labels_train, deep_features_val, labels_val)\n",
    "kNN_pca_val_acc = predict_and_find_accuracy(\"kNN\", pca_features_train, labels_train, pca_features_val, labels_val, best_pca_k)\n",
    "kNN_deep_val_acc = predict_and_find_accuracy(\"kNN\", deep_features_train, labels_train, deep_features_val, labels_val, best_deep_k)\n",
    "linear_pca_val_acc = predict_and_find_accuracy(\"linear\", pca_features_train, labels_train, pca_features_val, labels_val)\n",
    "linear_deep_val_acc = predict_and_find_accuracy(\"linear\", deep_features_train, labels_train, deep_features_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"      Model                      Train acc      Val acc     Test acc \")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"Naive Bayes with PCA features  :   \", \"{0:.03f}\".format(NB_pca_train_acc),  \"      \", \"{0:.03f}\".format(NB_pca_val_acc), \"      \", \"{0:.03f}\".format(NB_pca_test_acc))\n",
    "print(\"Naive Bayes with Deep features :   \", \"{0:.03f}\".format(NB_deep_train_acc),  \"      \", \"{0:.03f}\".format(NB_deep_val_acc), \"      \", \"{0:.03f}\".format(NB_deep_test_acc))\n",
    "print(\"kNN with PCA features          :   \", \"{0:.03f}\".format(kNN_pca_train_acc),  \"      \", \"{0:.03f}\".format(kNN_pca_val_acc), \"      \", \"{0:.03f}\".format(kNN_pca_test_acc))\n",
    "print(\"kNN with Deep features         :   \", \"{0:.03f}\".format(kNN_deep_train_acc),  \"      \", \"{0:.03f}\".format(kNN_deep_val_acc), \"      \", \"{0:.03f}\".format(kNN_deep_test_acc))\n",
    "print(\"Linear with PCA features       :   \", \"{0:.03f}\".format(linear_pca_train_acc),  \"      \", \"{0:.03f}\".format(linear_pca_val_acc), \"      \", \"{0:.03f}\".format(linear_pca_test_acc))\n",
    "print(\"Linear with Deep features      :   \", \"{0:.03f}\".format(linear_deep_train_acc),  \"      \", \"{0:.03f}\".format(linear_deep_val_acc), \"      \", \"{0:.03f}\".format(linear_deep_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in most cases, training accuracies are quite high, while validation and testing accuracies are not. Thus, a much better estimate of the test accuracy is the validation accuracy, rather than the training accuracy.\n",
    "\n",
    "### Low accuracies:\n",
    "\n",
    "As can be seen, the values of the validation accuracies do not go above 0.625, with PCA giving better accuracy than deep features. This is much better than the accuracy of a random guess (0.1), but it is quite less on an absolute level.\n",
    "\n",
    "**Reason:** We used **ONLY** 12 training images per class. That is not nearly enough to train any machine learning model!\n",
    "\n",
    "Machine learning models, especially deep models, always perform better with more data. This is why we have seen \"Big Data\" become popular - we now have access to large amounts of data, which is making all the machine learning models better than ever before.\n",
    "\n",
    "So, in real cases where we don't have a test set, we make a small validation data and find validation accuracy to choose our model. From the validation accuracies, we find that the **Linear Classifier** and **Naive Bayes Classifier** using **principal eigenfaces** perform better than the other models which hold for test accuracies too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "\n",
    "Through the above exercises, we learnt about the fundamental parts of a Machine Learning system. Apart from learning the importance of validation set, we revised a few concepts we have gone through in past few lectures such as PCA, Naive bayes, kNN, multi-class linear classifiers, and evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
