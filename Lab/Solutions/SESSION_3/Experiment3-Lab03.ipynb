{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3\n",
    "## Experiment 1\n",
    "### Lab3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After completing this exercise please complete the Check For Understanding questions in the LMS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Binary Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n",
    "In this experiment, we will use the CIFAR-10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images have been downloaded and unzipped for you in the directory cifar-10\n",
    "\n",
    "They are in a particular python-specific format called pickle. You need not worry about the format's internals, as the site has given the code needed to read such files. The code is given in the first code block below.\n",
    "\n",
    "**The code returns the contents of each data file as a dictionary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at the data\n",
    "\n",
    "There are 8 files in the cifar-10 directory.\n",
    "\n",
    "batches.meta\n",
    "\n",
    "data_batch_1\n",
    "\n",
    "data_batch_2\t\n",
    "\n",
    "data_batch_3\n",
    "\n",
    "data_batch_4\t\n",
    "\n",
    "data_batch_5\n",
    "\n",
    "readme.html\n",
    "\n",
    "test_batch\n",
    "\n",
    "We will take a peek at these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Special function to read special files\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**data** a 10,000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "\n",
    "**labels** a list of 10,000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class = horse\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e55e5d0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualize the images in CIFAR-10 Dataset\n",
    "## Here get_data unpickles the CIFAR Dataset and stores the data as 10000*3072 dimension in array X \n",
    "## and labels as 10000*1 dimension in array Y. \n",
    "## Visualize function shows the image corresponding to id number.\n",
    "\n",
    "def get_data(file):\n",
    "    dict = unpickle(file)\n",
    "    X = np.asarray(dict['data']).astype(\"uint8\")\n",
    "    Y = np.asarray(dict['labels'])\n",
    "    names = np.asarray(dict['filenames'])\n",
    "    list_class=(unpickle(\"../Datasets/cifar-10/batches.meta\")['label_names'])\n",
    "    return X,Y,names,list_class\n",
    "                     \n",
    "\n",
    "def visualize_image(X, Y, names, image_id):\n",
    "    rgb = X[image_id,:]\n",
    "    img = rgb.reshape(3, 32, 32).transpose([1, 2, 0])\n",
    "    print(img.shape)\n",
    "    plt.imshow(img)\n",
    "    plt.title(names[image_id])\n",
    "    plt.show()\n",
    "\n",
    "# Read image\n",
    "X, Y, names, classes = get_data(\"../Datasets/cifar-10/data_batch_3\")\n",
    "# Visualize the 10th image\n",
    "pick = 10\n",
    "print(\"Class =\",classes[Y[pick]])\n",
    "visualize_image(X, Y, names, pick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall run a linear classifier. You can look at the code that calculates weights for the optimal line in \"perceptron_sgd\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear classifier code -\n",
    "# code to estimate optimal linear boundary,\n",
    "# classify train data by estimating the optimal linear boundary,\n",
    "# predict labels based on linear boundary,\n",
    "# and compute the accuracy of the classification\n",
    "\n",
    "# code to estimate optimal linear boundary (can ignore for now),\n",
    "def perceptron_sgd(X, Y):\n",
    "    w = np.zeros(len(X[0]))\n",
    "    eta = 0.01 # learning rate\n",
    "    epochs = 100\n",
    "    for t in range(epochs):\n",
    "        if (t+1) % 50 == 0:\n",
    "            print(\"Running Epoch #\", t+1)\n",
    "            # print(\"acc:\", compute_accuracy(predict(X[:, :-1], w), Y))\n",
    "        for i, x in enumerate(X):\n",
    "            if (np.dot(X[i], w) * Y[i]) <= 0:\n",
    "                w = w + eta * X[i] * Y[i]\n",
    "        eta *= 0.75\n",
    "    return w\n",
    "\n",
    "# classify train data by estimating the optimal linear boundary,\n",
    "def classify(train_feat, train_labels):\n",
    "    ## mapping first label to -1 and second to +1\n",
    "    labels = np.sort(np.unique(train_labels))\n",
    "    lmap = {labels[0] : -1, labels[1] : 1}\n",
    "    l = [lmap[i] for i in train_labels]     \n",
    "    ## appending 1 to train features\n",
    "    add_one2train = np.ones((len(train_feat), 1))\n",
    "    append_train_features = np.hstack((np.asarray(train_feat), add_one2train))\n",
    "    w = perceptron_sgd(append_train_features, l)\n",
    "    return w\n",
    "\n",
    "# predict labels based on linear boundary,\n",
    "def predict(features, w):\n",
    "    ##appending 1 to test features\n",
    "    add_one = np.ones((len(features),1))\n",
    "    append_features = np.hstack((np.asarray(features), add_one))\n",
    "    pred = np.dot(append_features, w)\n",
    "    return pred\n",
    "\n",
    "# compute the accuracy of the classification\n",
    "def compute_accuracy(pred, test_labels):\n",
    "    # To make it general, let us find the unique set of labels in test_labels\n",
    "    # (could be \"apples\" and \"oranges\", or \"1\" and \"2\"),\n",
    "    labels = np.sort(np.unique(test_labels))\n",
    "    # and then assign -1 and 1 to these unique labels\n",
    "    lmap = {labels[0] : -1, labels[1] : 1}\n",
    "    # Let's now convert the labels to -1 and 1\n",
    "    l = [lmap[i] for i in test_labels]\n",
    "    # Let us find the accuracy\n",
    "    p = []\n",
    "    for i in range(len(pred)):\n",
    "        p.append(1 if pred[i] >= 0 else -1)\n",
    "    acc = np.mean(np.asarray(p) == np.asarray(l))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for binary classification\n",
    "def one_vs_one_classifier_ours(train_features, train_labels):\n",
    "    w = classify(train_features, train_labels)\n",
    "    return w\n",
    "\n",
    "def calc_accuracy(X_test, Y_test, w):\n",
    "    pred = predict(X_test, w)\n",
    "    accuracy = compute_accuracy(pred, Y_test)\n",
    "    print(\"accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "To study the effect of different features on binary classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature type 1: Raw Intensity values of RGB \n",
    "Use Intensity values of RGB as intensity features. For this use the raw intensity features extracted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unpickling the data and labels from CIFAR-10 Dataset,\n",
    "## and Preparing the raw features for training and test data.\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "# Read all training features and labels\n",
    "for j in \"12345\": \n",
    "    batch_file = '../Datasets/cifar-10/data_batch_'+ j\n",
    "    x_train, y_train, names_train, classes_train = get_data(batch_file)\n",
    "    X_train.extend(x_train)\n",
    "    Y_train.extend(y_train)\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)\n",
    "\n",
    "# Read all test features and labels\n",
    "X_test, Y_test, names_test, classes_test = get_data(\"../Datasets/cifar-10/test_batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, select only classes #5 and #7, and make a linear classifier for these two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_2classes(class0, class1, X, Y):\n",
    "    # Select class #0\n",
    "    X_0 = X[Y == class0]\n",
    "    Y_0 = Y[Y == class0]\n",
    "    # Select class #1\n",
    "    X_1 = X[Y == class1]\n",
    "    Y_1 = Y[Y == class1]\n",
    "    # Join the two classes to make the set\n",
    "    X_2classes = np.vstack((X_0, X_1))\n",
    "    Y_2classes = np.append(Y_0, Y_1)\n",
    "    return X_2classes, Y_2classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select classes #5 and #7\n",
    "X_train_2classes, Y_train_2classes = extract_2classes(5, 7, X_train, Y_train)\n",
    "X_test_2classes, Y_test_2classes = extract_2classes(5, 7, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "accuracy = 0.644\n"
     ]
    }
   ],
   "source": [
    "# Binary classification for classes 1 and 2 using hand features\n",
    "w = one_vs_one_classifier_ours(X_train_2classes, Y_train_2classes)\n",
    "\n",
    "# Find accuracy\n",
    "calc_accuracy(X_test_2classes, Y_test_2classes, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature type 2: Hand crafted features\n",
    "For this we extracted 9 dimension feature for each image. Feature consist of min,max and mean intensity values for RGB channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hand crafted features. For this we extracted 9 dimension feature for each image.\n",
    "## Feature consist of min, max and mean intensity values for RGB channel.\n",
    "\n",
    "# Extract min, max and mean of R, G, and B in each image\n",
    "# in train\n",
    "def extract_RGB_min_max_mean(X):\n",
    "    R, G, B = 1024, 2048, 3072\n",
    "    R_min = np.reshape(np.min(X[:, :R], axis=1), (len(X), 1))\n",
    "    R_max = np.reshape(np.max(X[:, :R], axis=1), (len(X), 1))\n",
    "    R_mean = np.reshape(np.mean(X[:, :R], axis=1), (len(X), 1))\n",
    "    G_min = np.reshape(np.min(X[:, R:G], axis=1), (len(X), 1))\n",
    "    G_max = np.reshape(np.max(X[:, R:G], axis=1), (len(X), 1))\n",
    "    G_mean = np.reshape(np.mean(X[:, R:G], axis=1), (len(X), 1))\n",
    "    B_min = np.reshape(np.min(X[:, G:B], axis=1), (len(X), 1))\n",
    "    B_max = np.reshape(np.max(X[:, G:B], axis=1), (len(X), 1))\n",
    "    B_mean = np.reshape(np.mean(X[:, G:B], axis=1), (len(X), 1))\n",
    "    return np.hstack((R_min, R_max, R_mean, G_min, G_max, G_mean, B_min, B_max, B_mean))\n",
    "\n",
    "head_features_train = extract_RGB_min_max_mean(X_train_2classes)\n",
    "head_features_test = extract_RGB_min_max_mean(X_test_2classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "accuracy = 0.5085\n"
     ]
    }
   ],
   "source": [
    "# Binary classification for classes 1 and 2 using hand features\n",
    "w = one_vs_one_classifier_ours(head_features_train, Y_train_2classes)\n",
    "\n",
    "# Find accuracy\n",
    "calc_accuracy(head_features_test, Y_test_2classes, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature-type3: PCA Features\n",
    "Use PCA to reduce features high dimensionality features into low dimansionality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply pca\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def extract_eigenvectors(k, X_train):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(X_train)\n",
    "    eigen_vectors = pca.components_\n",
    "    return eigen_vectors, pca\n",
    "\n",
    "def make_pca_features(eigen_vectors, X):\n",
    "    return np.transpose(np.dot(eigen_vectors, np.transpose(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with k = 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pca features\n",
    "k = 200\n",
    "eigen_vectors, pca_object = extract_eigenvectors(k, X_train_2classes)\n",
    "pca_features_train_2classes = make_pca_features(eigen_vectors, X_train_2classes)\n",
    "pca_features_test_2classes = make_pca_features(eigen_vectors, X_test_2classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "accuracy = 0.637\n"
     ]
    }
   ],
   "source": [
    "# Binary classification for classes 1 and 2 using hand features\n",
    "w = one_vs_one_classifier_ours(pca_features_train_2classes, Y_train_2classes)\n",
    "\n",
    "# Find accuracy\n",
    "calc_accuracy(pca_features_test_2classes, Y_test_2classes, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with k = 800\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "accuracy = 0.6435\n",
      "Training with k = 1000\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "accuracy = 0.6445\n",
      "Training with k = 1200\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "accuracy = 0.641\n",
      "Training with k = 1400\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "accuracy = 0.644\n"
     ]
    }
   ],
   "source": [
    "## Try different k values\n",
    "for k in [800, 1000, 1200, 1400]:\n",
    "    print(\"Training with k =\", k)\n",
    "    # Make pca features\n",
    "    eigen_vectors, pca_object = extract_eigenvectors(k, X_train_2classes)\n",
    "    pca_features_train = make_pca_features(eigen_vectors, X_train_2classes)\n",
    "    pca_features_test = make_pca_features(eigen_vectors, X_test_2classes)\n",
    "    # Binary classification for classes 1 and 2 using hand features\n",
    "    w = one_vs_one_classifier_ours(pca_features_train, Y_train_2classes)\n",
    "    # Find accuracy\n",
    "    calc_accuracy(pca_features_test, Y_test_2classes, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature type-4: Deep Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the deep features of images\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "features = sio.loadmat('../Datasets/cifar-10/cifar10_deep_features.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 512) (50000, 1) (10000, 512) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Extracting the deep features into training and testing\n",
    "deep_features_train = features['x_train']\n",
    "deep_labels_train = np.transpose(features['y_train'])\n",
    "deep_features_test = features['x_test']\n",
    "deep_labels_test = np.transpose(features['y_test'])\n",
    "print(deep_features_train.shape, deep_labels_train.shape, deep_features_test.shape, deep_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features of class0 and class1\n",
    "deep_features_train_2classes, deep_labels_train_2classes = extract_2classes(5, 7, deep_features_train, np.squeeze(deep_labels_train))\n",
    "deep_features_test_2classes, deep_labels_test_2classes = extract_2classes(5, 7, deep_features_test, np.squeeze(deep_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "accuracy = 0.9795\n"
     ]
    }
   ],
   "source": [
    "# Binary classification for classes 1 and 2 using hand features\n",
    "w = one_vs_one_classifier_ours(deep_features_train_2classes, deep_labels_train_2classes)\n",
    "\n",
    "# Find accuracy\n",
    "calc_accuracy(deep_features_test_2classes, deep_labels_test_2classes, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Multi Classification(One Vs One Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# Raw features\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hand features for full training and test sets\n",
    "head_features_train = extract_RGB_min_max_mean(X_train)\n",
    "head_features_test = extract_RGB_min_max_mean(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 9) (10000, 9)\n"
     ]
    }
   ],
   "source": [
    "print(head_features_train.shape, head_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pca features for full training and test sets\n",
    "# Make pca features, with k=200\n",
    "k = 200\n",
    "eigen_vectors, pca_object = extract_eigenvectors(k, X_train)\n",
    "pca_features_train = make_pca_features(eigen_vectors, X_train)\n",
    "pca_features_test = make_pca_features(eigen_vectors, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 200) (10000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(pca_features_train.shape, pca_features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 512) (10000, 512)\n"
     ]
    }
   ],
   "source": [
    "# Deep features\n",
    "print(deep_features_train.shape, deep_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the extracted features:\n",
    "\n",
    "1. Raw Features: (X_train, Y_train), (X_test, Y_test))\n",
    "2. Hand made features: (hand_features_train, Y_train), (hand_features_test, Y_test))\n",
    "3. PCA Features: (pca_features_train, Y_train), (pca_features_test, Y_test))\n",
    "4. Deep Features (VGG): (deep_features_train, deep_labels_train), (deep_features_test, deep_labels_test))\n",
    "                  \n",
    "Use respective features for One Vs One Multiclass classification Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import random\n",
    "import collections\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def extract_two_classes(data, x,y):\n",
    "    xtrain = []\n",
    "    ytrain = []\n",
    "    merged = []\n",
    "    merged.extend(data[x])\n",
    "    merged.extend(data[y])\n",
    "    random.shuffle(merged)\n",
    "    xtrain = list(zip(*merged))[0]\n",
    "    ytrain = list(zip(*merged))[1]\n",
    "    return xtrain, ytrain\n",
    "\n",
    "def oneVsone(data, num_classes, test_sample):\n",
    "    weight = []\n",
    "    prediction = []\n",
    "    for i, j in list(itertools.combinations(range(num_classes), 2)):\n",
    "        print(\"Training for classes\", i, j)\n",
    "        xtrain, ytrain = extract_two_classes(data, i,j)\n",
    "        w = classify(xtrain, ytrain)\n",
    "        weight.append((w,(i,j)))\n",
    "        pred = []\n",
    "        preds = predict(test_sample, w)\n",
    "        for p in predict(test_sample, w):\n",
    "            if p > 0:\n",
    "                pred.append(j)\n",
    "            else:\n",
    "                pred.append(i)\n",
    "        prediction.append(pred)\n",
    "        res = stats.mode(np.asarray(prediction))[0]\n",
    "    return np.squeeze(res)\n",
    "\n",
    "\n",
    "def multiclass_classification(X_train, Y_train, X_test, Y_test):\n",
    "\n",
    "    ## Train features and labels you want to use\n",
    "    xtrain = X_train\n",
    "    ytrain = Y_train\n",
    "    ## Test features and labels you want to check on\n",
    "    xtest = X_test\n",
    "    ytest = Y_test\n",
    "\n",
    "    l = zip(xtrain,  ytrain)\n",
    "    #L.sort(key=lambda x: x[1])\n",
    "    L = sorted(l, key=lambda x: x[1])\n",
    "\n",
    "    it = itertools.groupby(L, operator.itemgetter(1))\n",
    "    All_classes = []\n",
    "\n",
    "    for key, subiter in it:\n",
    "#         print ('Class:', key)\n",
    "        data = []\n",
    "        for item in subiter:\n",
    "            data.append(item)\n",
    "        All_classes.append(data)\n",
    "\n",
    "    pred = oneVsone(All_classes, len(np.unique(Y_train)), xtest)\n",
    "    print(accuracy_score(ytest, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES & LABELS\n",
    "train_features = deep_features_train\n",
    "train_labels = Y_train\n",
    "test_features = deep_features_test\n",
    "test_labels = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for classes 0 1\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 0 2\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 0 3\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 0 4\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 0 5\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 0 6\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 0 7\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 0 8\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 0 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 1 2\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 1 3\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 1 4\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 1 5\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 1 6\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 1 7\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 1 8\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 1 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 2 3\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 2 4\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 2 5\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 2 6\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 2 7\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 2 8\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 2 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 3 4\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 3 5\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 3 6\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 3 7\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 3 8\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 3 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 4 5\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 4 6\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 4 7\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 4 8\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 4 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 5 6\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 5 7\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 5 8\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 5 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 6 7\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 6 8\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 6 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 7 8\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 7 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "Training for classes 8 9\n",
      "Running Epoch # 50\n",
      "Running Epoch # 100\n",
      "0.9335\n"
     ]
    }
   ],
   "source": [
    "multiclass_classification(train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In case we want to use sklearn's library for *faster* multi-class linear classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 5 5\n",
      "  5 5 6 6 6 7 7 8]\n",
      " [1 2 3 4 5 6 7 8 9 2 3 4 5 6 7 8 9 3 4 5 6 7 8 9 4 5 6 7 8 9 5 6 7 8 9 6 7\n",
      "  8 9 7 8 9 8 9 9]]\n"
     ]
    }
   ],
   "source": [
    "actual_labels = np.array([(i, j) for i in range(10) for j in range(i+1, 10)]).T\n",
    "\n",
    "print(actual_labels)\n",
    "\n",
    "def extract_2classes_with_binary_labels(i, j, X_train, Y_train):\n",
    "    # Select class #0\n",
    "    X_0 = X[Y == i]\n",
    "    Y_0 = np.zeros((len(X_0)))\n",
    "    # Select class #1\n",
    "    X_1 = X[Y == j]\n",
    "    Y_1 = np.ones((len(X_1)))\n",
    "    # Join the two classes to make the set\n",
    "    X_2classes = np.vstack((X_0, X_1))\n",
    "    Y_2classes = np.append(Y_0, Y_1)\n",
    "    return X_2classes, Y_2classes\n",
    "\n",
    "# one-vs-one classifier\n",
    "from sklearn import linear_model\n",
    "def one_vs_one_classifier(train_features, train_labels):\n",
    "    clf = linear_model.SGDClassifier(random_state=1)\n",
    "    clf.fit(train_features, train_labels)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def multiclass_classify_using_sklearn(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    classifiers = []\n",
    "    \n",
    "    # For each pair of classes:\n",
    "    for i in range(0, 9):\n",
    "        for j in range(i+1, 10):\n",
    "            print(\"Training pair of classes:\", i, j)\n",
    "            \n",
    "            # Extract the train features and labels of the two classes\n",
    "            train_features, train_labels = extract_2classes_with_binary_labels(i, j, X_train, Y_train)\n",
    "            \n",
    "            # Let us make each one-vs-one classifier\n",
    "            # Train the classifier on these features and labels\n",
    "            clf = one_vs_one_classifier(train_features, train_labels)\n",
    "            classifiers.append(clf)\n",
    "    \n",
    "    # Find each classifier's prediction\n",
    "    predicted_classes_from_all_classifiers = np.zeros((len(X_test), len(classifiers)), dtype=int)\n",
    "    for c, clf in enumerate(classifiers):\n",
    "        preds = np.asarray(clf.predict(X_test), dtype=int)\n",
    "        predicted_classes_from_all_classifiers[:, c] = actual_labels[preds, c]\n",
    "\n",
    "    # Take majority vote for each sample\n",
    "    predicted_classes = []\n",
    "    for p in predicted_classes_from_all_classifiers:\n",
    "        predicted_classes.append(np.argmax(np.bincount(p)))\n",
    "    \n",
    "    # Find accuracy\n",
    "    test_accuracy = np.mean(predicted_classes == Y_test)\n",
    "    print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pair of classes: 0 1\n",
      "Training pair of classes: 0 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pair of classes: 0 3\n",
      "Training pair of classes: 0 4\n",
      "Training pair of classes: 0 5\n",
      "Training pair of classes: 0 6\n",
      "Training pair of classes: 0 7\n",
      "Training pair of classes: 0 8\n",
      "Training pair of classes: 0 9\n",
      "Training pair of classes: 1 2\n",
      "Training pair of classes: 1 3\n",
      "Training pair of classes: 1 4\n",
      "Training pair of classes: 1 5\n",
      "Training pair of classes: 1 6\n",
      "Training pair of classes: 1 7\n",
      "Training pair of classes: 1 8\n",
      "Training pair of classes: 1 9\n",
      "Training pair of classes: 2 3\n",
      "Training pair of classes: 2 4\n",
      "Training pair of classes: 2 5\n",
      "Training pair of classes: 2 6\n",
      "Training pair of classes: 2 7\n",
      "Training pair of classes: 2 8\n",
      "Training pair of classes: 2 9\n",
      "Training pair of classes: 3 4\n",
      "Training pair of classes: 3 5\n",
      "Training pair of classes: 3 6\n",
      "Training pair of classes: 3 7\n",
      "Training pair of classes: 3 8\n",
      "Training pair of classes: 3 9\n",
      "Training pair of classes: 4 5\n",
      "Training pair of classes: 4 6\n",
      "Training pair of classes: 4 7\n",
      "Training pair of classes: 4 8\n",
      "Training pair of classes: 4 9\n",
      "Training pair of classes: 5 6\n",
      "Training pair of classes: 5 7\n",
      "Training pair of classes: 5 8\n",
      "Training pair of classes: 5 9\n",
      "Training pair of classes: 6 7\n",
      "Training pair of classes: 6 8\n",
      "Training pair of classes: 6 9\n",
      "Training pair of classes: 7 8\n",
      "Training pair of classes: 7 9\n",
      "Training pair of classes: 8 9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 512 features per sample; expecting 3072",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-969b31a038dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmulticlass_classify_using_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-f199fff2209b>\u001b[0m in \u001b[0;36mmulticlass_classify_using_sklearn\u001b[0;34m(X_train, Y_train, X_test, Y_test)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpredicted_classes_from_all_classifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mpredicted_classes_from_all_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactual_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 512 features per sample; expecting 3072"
     ]
    }
   ],
   "source": [
    "multiclass_classify_using_sklearn(train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
