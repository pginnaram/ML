{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 06\n",
    "### Experiment 6 \n",
    "### Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we explore how k-Means clustering is able to find patterns and produce groupings without any form of external information to help.\n",
    "\n",
    "We take the text of the famous Russian novel \"War and Peace\" by Leo Tolstoy, downloaded from Project Gutenburg (http://www.gutenberg.org/), and extract all sentences from it. We use the nltk library to break the text into sentences. We build a word2vec representation using these sentences, after removing fluff words from these sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the experiment execute the following commands\n",
    "\n",
    "1.!apt-get update\n",
    "\n",
    "2.!pip install --upgrade gensim\n",
    "\n",
    "3.!pip install nltk\n",
    "\n",
    "4.import nltk\n",
    "\n",
    "5.nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "\n",
    "# !pip install --upgrade gensim\n",
    "\n",
    "# !pip install nltk\n",
    "\n",
    "# import nltk\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "We read the entire file into a list of lines, converting everything to lowercase as well as remove trailing and leading whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_text_stage0 = [line.strip().lower() for line in open(\"../Datasets/War_And_Peace.txt\",encoding=\"utf8\")]\n",
    "print(wp_text_stage0[4000:4010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "We combine them into one gigantic string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_text_stage1 = ' '.join(wp_text_stage0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wp_text_stage1))\n",
    "print(wp_text_stage1[40000:40200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "We break down this gigantic string into sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "wp_text_stage2 = sent_tokenize(wp_text_stage1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wp_text_stage2))\n",
    "print(wp_text_stage2[5000:5010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have about 26k sentences, in the tome. We now take each sentence and clean it up as below:\n",
    " * replace all non-alphanumeric characters by space\n",
    " * split each sentence on whitespace\n",
    " * in each sentence drop words that are less than 3 letters long and are part of fluff words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "We read the entire contents the fluff file into a set. As mentioned earlier a set is much faster for checking membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluff = set([line.strip() for line in open(\"../Datasets/stoplist.txt\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Replace all non-alphanumeric characters by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "only_alnum = re.compile(r\"[^\\w]+\") ## \\w => unicode alphabet\n",
    "#only_alnum = re.compile(r\"[^a-z0-9]\") --> This will remove accented characters which are part of many names!\n",
    "\n",
    "## Replaces one or more occurrence of any characters other unicode alphabets and numbers\n",
    "def cleanUp(s):\n",
    "    return re.sub(only_alnum, \" \", s).strip()\n",
    "wp_text_stage3 = [cleanUp(s) for s in wp_text_stage2]\n",
    "print(wp_text_stage3[4000:4010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Now we break each sentence into words, and store these words as a list. We traverse this list and drop the unwanted words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_words(s):\n",
    "    return [w for w in s.split() if len(w) > 2 and w not in fluff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_text_stage4 = [choose_words(sentence) for sentence in wp_text_stage3]\n",
    "print(wp_text_stage4[4000:4010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wp_text_stage4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "We convert the words to common stem -- that is we do not want to consider \"run\", \"runs\", \"running\" as separate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"), stemmer.stem(\"run\"), stemmer.stem(\"runs\"), stemmer.stem(\"runner\"))\n",
    "print(stemmer.stem(\"guns\"), stemmer.stem(\"gun\"), stemmer.stem(\"gunned\"), stemmer.stem(\"gunning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_list(wordlist):\n",
    "    return [stemmer.stem(word) for word in wordlist]\n",
    "for n in range(4000, 4010):\n",
    "    print(wp_text_stage4[n], stem_list(wp_text_stage4[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_text_stage5 = [stem_list(s) for s in wp_text_stage4]\n",
    "print(wp_text_stage5[4000:4010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "We now build a word2vec model with this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 50   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = word2vec.Word2Vec(wp_text_stage5, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wp.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(wp.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"chair\",\"car\",\"man\",\"woman\",\"clean\",\"close\",\"cloud\",\"coat\", \"confus\",\"danger\",\"daughter\",\"deal\",\"run\",\"walk\",\"count\",\"father\",\"girl\",\"near\",\"neck\",\"spoke\",\"spoken\",\"stand\",\"show\",\"shown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9\n",
    "Let us save this so that we can continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp.wv.save_word2vec_format('wp.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([wp[w] for w in wp.wv.vocab if w in words])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means\n",
    "K-means  is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed a priori. The main idea is to define k centroids, one for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 4)\n",
    "km.fit(X)\n",
    "y_kmeans = km.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "lle_data = manifold.LocallyLinearEmbedding(n_neighbors=10, n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(lle_data[:,0],lle_data[:,1], c =y_kmeans )\n",
    "for i in range(len(words)-1):\n",
    "    plt.annotate(words[i], xy = (lle_data[i][0],lle_data[i][1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words get divided into four clusters  as shown by the four colors, visualize into 2D by LLE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
