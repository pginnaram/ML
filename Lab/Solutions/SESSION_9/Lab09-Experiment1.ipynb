{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 09\n",
    "### Experiment 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "1. A replacement for NumPy to use the power of GPUs\n",
    "\n",
    "2. a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\n",
    "\n",
    "In this experiment we will use MNIST dataset and will be implementing MLP using Pytorch. We are going to do this step-by-step\n",
    "\n",
    "1. Loading MNIST dataset and Visualize\n",
    "2. Defining Loss functions\n",
    "3. Doing forward pass\n",
    "4. Run the classifier the complete test set and compute accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the pytorch run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==0.3.1 from http://download.pytorch.org/whl/cpu/torch-0.3.1-cp35-cp35m-linux_x86_64.whl in /home/tsuser/.local/lib/python3.5/site-packages\r\n",
      "Requirement already satisfied: numpy in /home/tsuser/.local/lib/python3.5/site-packages (from torch==0.3.1)\r\n",
      "Requirement already satisfied: pyyaml in /home/tsuser/.local/lib/python3.5/site-packages (from torch==0.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.3.1-cp35-cp35m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required pytorch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the MNIST data. First time we may have to download the data, which can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the train set file\n",
    "train_dataset = dsets.MNIST(root='../data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "#Loading the test set file\n",
    "test_dataset = dsets.MNIST(root='../data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# loading the test dataset\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. Loading X and Y train values from the loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([10, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([10]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting first 10 training digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the network as a Python class. We have to write the __init__() and forward() methods, and PyTorch will automatically generate a backward() method for computing the gradients for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a neural network object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CrossEntropyLoss function uses inputs, labels  to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/6000], Loss: 0.6394\n",
      "Epoch [1/5], Step [200/6000], Loss: 0.3461\n",
      "Epoch [1/5], Step [300/6000], Loss: 0.2559\n",
      "Epoch [1/5], Step [400/6000], Loss: 0.1266\n",
      "Epoch [1/5], Step [500/6000], Loss: 0.5654\n",
      "Epoch [1/5], Step [600/6000], Loss: 0.0597\n",
      "Epoch [1/5], Step [700/6000], Loss: 0.1744\n",
      "Epoch [1/5], Step [800/6000], Loss: 0.0764\n",
      "Epoch [1/5], Step [900/6000], Loss: 0.1121\n",
      "Epoch [1/5], Step [1000/6000], Loss: 0.1119\n",
      "Epoch [1/5], Step [1100/6000], Loss: 0.0827\n",
      "Epoch [1/5], Step [1200/6000], Loss: 0.0554\n",
      "Epoch [1/5], Step [1300/6000], Loss: 0.0563\n",
      "Epoch [1/5], Step [1400/6000], Loss: 0.0551\n",
      "Epoch [1/5], Step [1500/6000], Loss: 0.4168\n",
      "Epoch [1/5], Step [1600/6000], Loss: 0.0142\n",
      "Epoch [1/5], Step [1700/6000], Loss: 0.0950\n",
      "Epoch [1/5], Step [1800/6000], Loss: 0.4939\n",
      "Epoch [1/5], Step [1900/6000], Loss: 0.8642\n",
      "Epoch [1/5], Step [2000/6000], Loss: 0.0408\n",
      "Epoch [1/5], Step [2100/6000], Loss: 0.3183\n",
      "Epoch [1/5], Step [2200/6000], Loss: 0.3487\n",
      "Epoch [1/5], Step [2300/6000], Loss: 0.2130\n",
      "Epoch [1/5], Step [2400/6000], Loss: 0.2086\n",
      "Epoch [1/5], Step [2500/6000], Loss: 0.0306\n",
      "Epoch [1/5], Step [2600/6000], Loss: 0.0733\n",
      "Epoch [1/5], Step [2700/6000], Loss: 0.1205\n",
      "Epoch [1/5], Step [2800/6000], Loss: 0.0122\n",
      "Epoch [1/5], Step [2900/6000], Loss: 0.0642\n",
      "Epoch [1/5], Step [3000/6000], Loss: 0.1583\n",
      "Epoch [1/5], Step [3100/6000], Loss: 0.0029\n",
      "Epoch [1/5], Step [3200/6000], Loss: 0.0590\n",
      "Epoch [1/5], Step [3300/6000], Loss: 0.0115\n",
      "Epoch [1/5], Step [3400/6000], Loss: 0.0425\n",
      "Epoch [1/5], Step [3500/6000], Loss: 0.0077\n",
      "Epoch [1/5], Step [3600/6000], Loss: 0.0183\n",
      "Epoch [1/5], Step [3700/6000], Loss: 0.0247\n",
      "Epoch [1/5], Step [3800/6000], Loss: 0.1260\n",
      "Epoch [1/5], Step [3900/6000], Loss: 0.0222\n",
      "Epoch [1/5], Step [4000/6000], Loss: 0.1966\n",
      "Epoch [1/5], Step [4100/6000], Loss: 0.0051\n",
      "Epoch [1/5], Step [4200/6000], Loss: 0.0290\n",
      "Epoch [1/5], Step [4300/6000], Loss: 0.0181\n",
      "Epoch [1/5], Step [4400/6000], Loss: 0.3265\n",
      "Epoch [1/5], Step [4500/6000], Loss: 0.2935\n",
      "Epoch [1/5], Step [4600/6000], Loss: 0.0018\n",
      "Epoch [1/5], Step [4700/6000], Loss: 0.0643\n",
      "Epoch [1/5], Step [4800/6000], Loss: 0.0176\n",
      "Epoch [1/5], Step [4900/6000], Loss: 0.0080\n",
      "Epoch [1/5], Step [5000/6000], Loss: 0.3481\n",
      "Epoch [1/5], Step [5100/6000], Loss: 0.0354\n",
      "Epoch [1/5], Step [5200/6000], Loss: 0.0277\n",
      "Epoch [1/5], Step [5300/6000], Loss: 0.0060\n",
      "Epoch [1/5], Step [5400/6000], Loss: 0.3430\n",
      "Epoch [1/5], Step [5500/6000], Loss: 0.0250\n",
      "Epoch [1/5], Step [5600/6000], Loss: 0.0068\n",
      "Epoch [1/5], Step [5700/6000], Loss: 0.0693\n",
      "Epoch [1/5], Step [5800/6000], Loss: 0.6055\n",
      "Epoch [1/5], Step [5900/6000], Loss: 0.0585\n",
      "Epoch [1/5], Step [6000/6000], Loss: 0.0721\n",
      "Epoch [2/5], Step [100/6000], Loss: 0.0581\n",
      "Epoch [2/5], Step [200/6000], Loss: 0.0263\n",
      "Epoch [2/5], Step [300/6000], Loss: 0.0013\n",
      "Epoch [2/5], Step [400/6000], Loss: 0.0016\n",
      "Epoch [2/5], Step [500/6000], Loss: 0.4035\n",
      "Epoch [2/5], Step [600/6000], Loss: 0.0008\n",
      "Epoch [2/5], Step [700/6000], Loss: 0.2027\n",
      "Epoch [2/5], Step [800/6000], Loss: 0.1637\n",
      "Epoch [2/5], Step [900/6000], Loss: 0.0024\n",
      "Epoch [2/5], Step [1000/6000], Loss: 0.0424\n",
      "Epoch [2/5], Step [1100/6000], Loss: 0.0014\n",
      "Epoch [2/5], Step [1200/6000], Loss: 0.0982\n",
      "Epoch [2/5], Step [1300/6000], Loss: 0.0040\n",
      "Epoch [2/5], Step [1400/6000], Loss: 0.0161\n",
      "Epoch [2/5], Step [1500/6000], Loss: 0.0036\n",
      "Epoch [2/5], Step [1600/6000], Loss: 0.0104\n",
      "Epoch [2/5], Step [1700/6000], Loss: 0.0012\n",
      "Epoch [2/5], Step [1800/6000], Loss: 0.0082\n",
      "Epoch [2/5], Step [1900/6000], Loss: 0.0012\n",
      "Epoch [2/5], Step [2000/6000], Loss: 0.0321\n",
      "Epoch [2/5], Step [2100/6000], Loss: 0.0142\n",
      "Epoch [2/5], Step [2200/6000], Loss: 0.0020\n",
      "Epoch [2/5], Step [2300/6000], Loss: 0.0062\n",
      "Epoch [2/5], Step [2400/6000], Loss: 0.0772\n",
      "Epoch [2/5], Step [2500/6000], Loss: 0.0001\n",
      "Epoch [2/5], Step [2600/6000], Loss: 0.0002\n",
      "Epoch [2/5], Step [2700/6000], Loss: 0.0069\n",
      "Epoch [2/5], Step [2800/6000], Loss: 0.0013\n",
      "Epoch [2/5], Step [2900/6000], Loss: 0.0874\n",
      "Epoch [2/5], Step [3000/6000], Loss: 0.0340\n",
      "Epoch [2/5], Step [3100/6000], Loss: 0.1317\n",
      "Epoch [2/5], Step [3200/6000], Loss: 0.1054\n",
      "Epoch [2/5], Step [3300/6000], Loss: 0.0058\n",
      "Epoch [2/5], Step [3400/6000], Loss: 0.0295\n",
      "Epoch [2/5], Step [3500/6000], Loss: 0.0122\n",
      "Epoch [2/5], Step [3600/6000], Loss: 0.0012\n",
      "Epoch [2/5], Step [3700/6000], Loss: 0.0212\n",
      "Epoch [2/5], Step [3800/6000], Loss: 0.0006\n",
      "Epoch [2/5], Step [3900/6000], Loss: 0.0046\n",
      "Epoch [2/5], Step [4000/6000], Loss: 0.4187\n",
      "Epoch [2/5], Step [4100/6000], Loss: 0.0022\n",
      "Epoch [2/5], Step [4200/6000], Loss: 0.1436\n",
      "Epoch [2/5], Step [4300/6000], Loss: 0.1501\n",
      "Epoch [2/5], Step [4400/6000], Loss: 0.7000\n",
      "Epoch [2/5], Step [4500/6000], Loss: 0.0752\n",
      "Epoch [2/5], Step [4600/6000], Loss: 0.0072\n",
      "Epoch [2/5], Step [4700/6000], Loss: 0.0043\n",
      "Epoch [2/5], Step [4800/6000], Loss: 0.0004\n",
      "Epoch [2/5], Step [4900/6000], Loss: 0.0848\n",
      "Epoch [2/5], Step [5000/6000], Loss: 0.0571\n",
      "Epoch [2/5], Step [5100/6000], Loss: 0.2380\n",
      "Epoch [2/5], Step [5200/6000], Loss: 0.0406\n",
      "Epoch [2/5], Step [5300/6000], Loss: 0.0395\n",
      "Epoch [2/5], Step [5400/6000], Loss: 0.1744\n",
      "Epoch [2/5], Step [5500/6000], Loss: 0.1293\n",
      "Epoch [2/5], Step [5600/6000], Loss: 0.0052\n",
      "Epoch [2/5], Step [5700/6000], Loss: 0.0543\n",
      "Epoch [2/5], Step [5800/6000], Loss: 0.1973\n",
      "Epoch [2/5], Step [5900/6000], Loss: 0.0005\n",
      "Epoch [2/5], Step [6000/6000], Loss: 0.0009\n",
      "Epoch [3/5], Step [100/6000], Loss: 0.0062\n",
      "Epoch [3/5], Step [200/6000], Loss: 0.0754\n",
      "Epoch [3/5], Step [300/6000], Loss: 0.0113\n",
      "Epoch [3/5], Step [400/6000], Loss: 0.0214\n",
      "Epoch [3/5], Step [500/6000], Loss: 0.0062\n",
      "Epoch [3/5], Step [600/6000], Loss: 0.0022\n",
      "Epoch [3/5], Step [700/6000], Loss: 0.0781\n",
      "Epoch [3/5], Step [800/6000], Loss: 0.0175\n",
      "Epoch [3/5], Step [900/6000], Loss: 0.0228\n",
      "Epoch [3/5], Step [1000/6000], Loss: 0.0012\n",
      "Epoch [3/5], Step [1100/6000], Loss: 0.0435\n",
      "Epoch [3/5], Step [1200/6000], Loss: 0.0151\n",
      "Epoch [3/5], Step [1300/6000], Loss: 0.0653\n",
      "Epoch [3/5], Step [1400/6000], Loss: 0.0034\n",
      "Epoch [3/5], Step [1500/6000], Loss: 0.0249\n",
      "Epoch [3/5], Step [1600/6000], Loss: 0.0011\n",
      "Epoch [3/5], Step [1700/6000], Loss: 0.0048\n",
      "Epoch [3/5], Step [1800/6000], Loss: 0.0336\n",
      "Epoch [3/5], Step [1900/6000], Loss: 0.0012\n",
      "Epoch [3/5], Step [2000/6000], Loss: 0.0116\n",
      "Epoch [3/5], Step [2100/6000], Loss: 0.0093\n",
      "Epoch [3/5], Step [2200/6000], Loss: 0.0459\n",
      "Epoch [3/5], Step [2300/6000], Loss: 0.0002\n",
      "Epoch [3/5], Step [2400/6000], Loss: 0.0257\n",
      "Epoch [3/5], Step [2500/6000], Loss: 0.0000\n",
      "Epoch [3/5], Step [2600/6000], Loss: 0.0011\n",
      "Epoch [3/5], Step [2700/6000], Loss: 0.0154\n",
      "Epoch [3/5], Step [2800/6000], Loss: 0.1849\n",
      "Epoch [3/5], Step [2900/6000], Loss: 0.0003\n",
      "Epoch [3/5], Step [3000/6000], Loss: 0.0002\n",
      "Epoch [3/5], Step [3100/6000], Loss: 0.0005\n",
      "Epoch [3/5], Step [3200/6000], Loss: 0.0211\n",
      "Epoch [3/5], Step [3300/6000], Loss: 0.0156\n",
      "Epoch [3/5], Step [3400/6000], Loss: 0.0015\n",
      "Epoch [3/5], Step [3500/6000], Loss: 0.0278\n",
      "Epoch [3/5], Step [3600/6000], Loss: 0.0009\n",
      "Epoch [3/5], Step [3700/6000], Loss: 0.0001\n",
      "Epoch [3/5], Step [3800/6000], Loss: 0.0293\n",
      "Epoch [3/5], Step [3900/6000], Loss: 0.0022\n",
      "Epoch [3/5], Step [4000/6000], Loss: 0.0004\n",
      "Epoch [3/5], Step [4100/6000], Loss: 0.1567\n",
      "Epoch [3/5], Step [4200/6000], Loss: 0.0075\n",
      "Epoch [3/5], Step [4300/6000], Loss: 0.0016\n",
      "Epoch [3/5], Step [4400/6000], Loss: 0.0015\n",
      "Epoch [3/5], Step [4500/6000], Loss: 0.0568\n",
      "Epoch [3/5], Step [4600/6000], Loss: 0.2797\n",
      "Epoch [3/5], Step [4700/6000], Loss: 0.0838\n",
      "Epoch [3/5], Step [4800/6000], Loss: 0.0085\n",
      "Epoch [3/5], Step [4900/6000], Loss: 0.0455\n",
      "Epoch [3/5], Step [5000/6000], Loss: 0.0001\n",
      "Epoch [3/5], Step [5100/6000], Loss: 0.0052\n",
      "Epoch [3/5], Step [5200/6000], Loss: 0.0025\n",
      "Epoch [3/5], Step [5300/6000], Loss: 0.0002\n",
      "Epoch [3/5], Step [5400/6000], Loss: 0.0124\n",
      "Epoch [3/5], Step [5500/6000], Loss: 0.0893\n",
      "Epoch [3/5], Step [5600/6000], Loss: 0.0335\n",
      "Epoch [3/5], Step [5700/6000], Loss: 0.0012\n",
      "Epoch [3/5], Step [5800/6000], Loss: 0.0007\n",
      "Epoch [3/5], Step [5900/6000], Loss: 0.0008\n",
      "Epoch [3/5], Step [6000/6000], Loss: 0.0058\n",
      "Epoch [4/5], Step [100/6000], Loss: 0.0006\n",
      "Epoch [4/5], Step [200/6000], Loss: 0.0052\n",
      "Epoch [4/5], Step [300/6000], Loss: 0.1295\n",
      "Epoch [4/5], Step [400/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [500/6000], Loss: 0.1534\n",
      "Epoch [4/5], Step [600/6000], Loss: 0.0012\n",
      "Epoch [4/5], Step [700/6000], Loss: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [800/6000], Loss: 0.0080\n",
      "Epoch [4/5], Step [900/6000], Loss: 0.0028\n",
      "Epoch [4/5], Step [1000/6000], Loss: 0.0325\n",
      "Epoch [4/5], Step [1100/6000], Loss: 0.0004\n",
      "Epoch [4/5], Step [1200/6000], Loss: 0.0076\n",
      "Epoch [4/5], Step [1300/6000], Loss: 0.0102\n",
      "Epoch [4/5], Step [1400/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [1500/6000], Loss: 0.4184\n",
      "Epoch [4/5], Step [1600/6000], Loss: 0.0126\n",
      "Epoch [4/5], Step [1700/6000], Loss: 0.2301\n",
      "Epoch [4/5], Step [1800/6000], Loss: 0.0009\n",
      "Epoch [4/5], Step [1900/6000], Loss: 0.0191\n",
      "Epoch [4/5], Step [2000/6000], Loss: 0.0094\n",
      "Epoch [4/5], Step [2100/6000], Loss: 0.0263\n",
      "Epoch [4/5], Step [2200/6000], Loss: 0.0002\n",
      "Epoch [4/5], Step [2300/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [2400/6000], Loss: 0.0010\n",
      "Epoch [4/5], Step [2500/6000], Loss: 0.0261\n",
      "Epoch [4/5], Step [2600/6000], Loss: 0.0004\n",
      "Epoch [4/5], Step [2700/6000], Loss: 0.0149\n",
      "Epoch [4/5], Step [2800/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [2900/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [3000/6000], Loss: 0.0631\n",
      "Epoch [4/5], Step [3100/6000], Loss: 0.0031\n",
      "Epoch [4/5], Step [3200/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [3300/6000], Loss: 0.0410\n",
      "Epoch [4/5], Step [3400/6000], Loss: 0.0140\n",
      "Epoch [4/5], Step [3500/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [3600/6000], Loss: 0.0182\n",
      "Epoch [4/5], Step [3700/6000], Loss: 0.0038\n",
      "Epoch [4/5], Step [3800/6000], Loss: 0.0027\n",
      "Epoch [4/5], Step [3900/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [4000/6000], Loss: 0.0006\n",
      "Epoch [4/5], Step [4100/6000], Loss: 0.0604\n",
      "Epoch [4/5], Step [4200/6000], Loss: 0.0064\n",
      "Epoch [4/5], Step [4300/6000], Loss: 0.0000\n",
      "Epoch [4/5], Step [4400/6000], Loss: 0.0001\n",
      "Epoch [4/5], Step [4500/6000], Loss: 0.0021\n",
      "Epoch [4/5], Step [4600/6000], Loss: 0.0083\n",
      "Epoch [4/5], Step [4700/6000], Loss: 0.0002\n",
      "Epoch [4/5], Step [4800/6000], Loss: 0.3616\n",
      "Epoch [4/5], Step [4900/6000], Loss: 0.0102\n",
      "Epoch [4/5], Step [5000/6000], Loss: 0.0038\n",
      "Epoch [4/5], Step [5100/6000], Loss: 0.0046\n",
      "Epoch [4/5], Step [5200/6000], Loss: 0.1893\n",
      "Epoch [4/5], Step [5300/6000], Loss: 0.0004\n",
      "Epoch [4/5], Step [5400/6000], Loss: 0.0202\n",
      "Epoch [4/5], Step [5500/6000], Loss: 0.0003\n",
      "Epoch [4/5], Step [5600/6000], Loss: 0.0002\n",
      "Epoch [4/5], Step [5700/6000], Loss: 0.0319\n",
      "Epoch [4/5], Step [5800/6000], Loss: 0.0005\n",
      "Epoch [4/5], Step [5900/6000], Loss: 0.0026\n",
      "Epoch [4/5], Step [6000/6000], Loss: 0.0006\n",
      "Epoch [5/5], Step [100/6000], Loss: 0.0017\n",
      "Epoch [5/5], Step [200/6000], Loss: 0.0021\n",
      "Epoch [5/5], Step [300/6000], Loss: 0.0320\n",
      "Epoch [5/5], Step [400/6000], Loss: 0.0056\n",
      "Epoch [5/5], Step [500/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [600/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [700/6000], Loss: 0.0181\n",
      "Epoch [5/5], Step [800/6000], Loss: 0.0240\n",
      "Epoch [5/5], Step [900/6000], Loss: 0.2651\n",
      "Epoch [5/5], Step [1000/6000], Loss: 0.0012\n",
      "Epoch [5/5], Step [1100/6000], Loss: 0.0242\n",
      "Epoch [5/5], Step [1200/6000], Loss: 0.0017\n",
      "Epoch [5/5], Step [1300/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [1400/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [1500/6000], Loss: 0.0162\n",
      "Epoch [5/5], Step [1600/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [1700/6000], Loss: 0.0003\n",
      "Epoch [5/5], Step [1800/6000], Loss: 0.0080\n",
      "Epoch [5/5], Step [1900/6000], Loss: 0.0656\n",
      "Epoch [5/5], Step [2000/6000], Loss: 0.0015\n",
      "Epoch [5/5], Step [2100/6000], Loss: 0.0039\n",
      "Epoch [5/5], Step [2200/6000], Loss: 0.2357\n",
      "Epoch [5/5], Step [2300/6000], Loss: 0.0675\n",
      "Epoch [5/5], Step [2400/6000], Loss: 0.0019\n",
      "Epoch [5/5], Step [2500/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [2600/6000], Loss: 0.0014\n",
      "Epoch [5/5], Step [2700/6000], Loss: 0.0938\n",
      "Epoch [5/5], Step [2800/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [2900/6000], Loss: 0.0012\n",
      "Epoch [5/5], Step [3000/6000], Loss: 0.0005\n",
      "Epoch [5/5], Step [3100/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [3200/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [3300/6000], Loss: 0.0005\n",
      "Epoch [5/5], Step [3400/6000], Loss: 0.0080\n",
      "Epoch [5/5], Step [3500/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [3600/6000], Loss: 0.0577\n",
      "Epoch [5/5], Step [3700/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [3800/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [3900/6000], Loss: 0.0027\n",
      "Epoch [5/5], Step [4000/6000], Loss: 0.0021\n",
      "Epoch [5/5], Step [4100/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [4200/6000], Loss: 0.0009\n",
      "Epoch [5/5], Step [4300/6000], Loss: 0.0003\n",
      "Epoch [5/5], Step [4400/6000], Loss: 0.0035\n",
      "Epoch [5/5], Step [4500/6000], Loss: 0.0009\n",
      "Epoch [5/5], Step [4600/6000], Loss: 0.0001\n",
      "Epoch [5/5], Step [4700/6000], Loss: 0.0017\n",
      "Epoch [5/5], Step [4800/6000], Loss: 0.0006\n",
      "Epoch [5/5], Step [4900/6000], Loss: 0.3630\n",
      "Epoch [5/5], Step [5000/6000], Loss: 0.7539\n",
      "Epoch [5/5], Step [5100/6000], Loss: 0.0005\n",
      "Epoch [5/5], Step [5200/6000], Loss: 0.1473\n",
      "Epoch [5/5], Step [5300/6000], Loss: 0.0076\n",
      "Epoch [5/5], Step [5400/6000], Loss: 0.0002\n",
      "Epoch [5/5], Step [5500/6000], Loss: 0.2332\n",
      "Epoch [5/5], Step [5600/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [5700/6000], Loss: 0.0434\n",
      "Epoch [5/5], Step [5800/6000], Loss: 0.0017\n",
      "Epoch [5/5], Step [5900/6000], Loss: 0.0000\n",
      "Epoch [5/5], Step [6000/6000], Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        inputs = net(images)\n",
    "        loss = criterion(inputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "\n",
    "Change the number of epochs to 10 and batch size to 50. Check the output for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
