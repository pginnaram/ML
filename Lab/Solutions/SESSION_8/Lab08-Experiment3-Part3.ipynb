{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 08\n",
    "### Experiment 3 Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "\n",
    "In this form of cross-validation the data set is divided into k subsets. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. Then the average error across all k trials is computed. \n",
    "\n",
    "In this experiment we are going to apply k-fold cross-validation method on the MNIST datasets and then tune the hyper parameters of MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': \"Optical Recognition of Handwritten Digits Data Set\\n===================================================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\nReferences\\n----------\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\",\n",
       " 'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading MNIST dataset from sklearn\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "## Loding the data and storing in x\n",
    "X = digits.data\n",
    "## Loading the target data and storing it in y\n",
    "y = digits.target\n",
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyper parameters\n",
    "# activation\n",
    "a = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
    "#solvers\n",
    "s = [\"lbfgs\",\"sgd\",\"adam\"]\n",
    "#learning rate\n",
    "lr = [0.0001,0.001,0.01,0.1]\n",
    "#hidden layers\n",
    "h = [(5,2),(3,2),(6,3),(7,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying K-Folds cross-validator\n",
    "kf = KFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to Create MLP classifier object with hyper parameters\n",
    "def mlp(a,s,h,lr):\n",
    "    clf = MLPClassifier(activation= a ,solver= s ,hidden_layer_sizes = h,max_iter = 5000 ,learning_rate = 'constant',learning_rate_init=lr)\n",
    "    return clf  \n",
    "#function to calculate the accuracy\n",
    "def accuracy(actual,predicted):\n",
    "    return np.count_nonzero(actual == predicted)*1.0/len(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 1** Predict the values using test data and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (3, 2)\n",
      "(train,test) accuracy =  0.36856549007686057 0.3561804008908686\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "(train,test) accuracy =  0.6848684514879753 0.5820217767879238\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  adam \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.4269468199471735 0.3902548874041079\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  adam \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "(train,test) accuracy =  0.8842311411885737 0.7390435535758476\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "(train,test) accuracy =  0.643255619147066 0.5365392229646128\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  adam \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  0.7258352719197954 0.6471690175699085\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  adam \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  0.6483017266637148 0.5564625092798813\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (3, 2)\n",
      "(train,test) accuracy =  0.39788137833497456 0.36393466963622867\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (3, 2)\n",
      "(train,test) accuracy =  0.26547358235357615 0.25310566691413017\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  0.5347534305270091 0.48590200445434306\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "for i in range(10):\n",
    "    k1 = np.random.randint(0,len(a))\n",
    "    k2 = np.random.randint(0,len(s))\n",
    "    k3 = np.random.randint(0,len(lr))\n",
    "    k4 = np.random.randint(0,len(h))\n",
    "    print(\"\\nHyper-parameters = \\n activation = \", a[k1],    \"\\n solver = \", s[k2], \"\\n learning_rate_init = \", lr[k3],         \"\\n hidden_layer_sizes = \", h[k4])\n",
    "     #calling the mlp function with random hyper paramters\n",
    "    clf = mlp(a[k1],s[k2],h[k4],lr[k3])\n",
    "    tempTrain = 0\n",
    "    tempTest = 0#In this experiment we are going to apply leave one out method on the MNIST datasets then we tune the hyper parameters of MulitiLayer perceptron classifier.\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        ## Splitting the data into train and test\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test  = y[train_index], y[test_index]\n",
    "        ##fit the data into the model\n",
    "        clf.fit(X_train,Y_train)\n",
    "        ##predicting the values on the fitted model using train data\n",
    "        predTrain = clf.predict((X_train))\n",
    "        #adding the accuracy\n",
    "        tempTrain = tempTrain + accuracy(Y_train,predTrain)\n",
    "        ##predict the values on the fitted model using test data\n",
    "        predTest = clf.predict((X_test))\n",
    "        #adding the accuracy\n",
    "        tempTest =  tempTest + accuracy(Y_test,predTest)\n",
    "    ##Calculating the train accuracy\n",
    "    train_accuracy.append(tempTrain*1.0/4)\n",
    "    ##Calculating the test accuracy\n",
    "    test_accuracy.append(tempTest*1.0/4)\n",
    "    print(\"(train,test) accuracy = \",tempTrain*1.0/4, tempTest*1.0/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Plotting the data\n",
    "xx = np.array(range(1,11))\n",
    "plt.bar(xx-0.2,train_accuracy,width=0.2)\n",
    "plt.bar(xx, test_accuracy,width=0.2)\n",
    "plt.legend([\"Train\",\"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 ** Vary the number of k-fold splits and observe the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading MNIST dataset from sklearn\n",
    "digits = datasets.load_digits(n_class=9)\n",
    "## Loding the data and storing in x\n",
    "X = digits.data\n",
    "## Loading the target data and storing it in y\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyper parameters\n",
    "# activation\n",
    "a = [\"identity\",\"logistic\",\"tanh\",\"relu\"]\n",
    "#solvers\n",
    "s = [\"lbfgs\",\"sgd\",\"adam\"]\n",
    "#learning rate\n",
    "lr = [0.0001,0.001,0.01,0.1]\n",
    "#hidden layers\n",
    "h = [(5,2),(3,2),(6,3),(7,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying K-Folds cross-validator\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to Create MLP classifier object with hyper parameters\n",
    "def mlp(a,s,h,lr):\n",
    "    clf = MLPClassifier(activation= a ,solver= s ,hidden_layer_sizes = h,max_iter = 5000 ,learning_rate = 'constant',learning_rate_init=lr)\n",
    "    return clf  \n",
    "#function to calculate the accuracy\n",
    "def accuracy(actual,predicted):\n",
    "    return np.count_nonzero(actual == predicted)*1.0/len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.6620251000811648 0.5792865879295188\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (3, 2)\n",
      "(train,test) accuracy =  0.24274329375510267 0.2272531819745442\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  0.9034766624709678 0.8116662844475022\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  adam \n",
      " learning_rate_init =  0.0001 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.5480990256654845 0.5101407522073156\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  tanh \n",
      " solver =  adam \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.7104389525814305 0.6586735083897106\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.001 \n",
      " hidden_layer_sizes =  (3, 2)\n",
      "(train,test) accuracy =  0.6587918120518163 0.5551542254328632\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  identity \n",
      " solver =  adam \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (5, 2)\n",
      "(train,test) accuracy =  0.9939380518808326 0.8696441539578794\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  lbfgs \n",
      " learning_rate_init =  0.1 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "(train,test) accuracy =  0.9647274409464349 0.8611875549440049\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  relu \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (7, 2)\n",
      "(train,test) accuracy =  0.3539134753655099 0.3129108856018041\n",
      "\n",
      "Hyper-parameters = \n",
      " activation =  logistic \n",
      " solver =  sgd \n",
      " learning_rate_init =  0.01 \n",
      " hidden_layer_sizes =  (6, 3)\n",
      "(train,test) accuracy =  0.9730151415719646 0.8721142453082598\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "for i in range(10):\n",
    "    k1 = np.random.randint(0,len(a))\n",
    "    k2 = np.random.randint(0,len(s))\n",
    "    k3 = np.random.randint(0,len(lr))\n",
    "    k4 = np.random.randint(0,len(h))\n",
    "    print(\"\\nHyper-parameters = \\n activation = \", a[k1],    \"\\n solver = \", s[k2], \"\\n learning_rate_init = \", lr[k3],         \"\\n hidden_layer_sizes = \", h[k4])\n",
    "     #calling the mlp function with random hyper paramters\n",
    "    clf = mlp(a[k1],s[k2],h[k4],lr[k3])\n",
    "    tempTrain = 0\n",
    "    tempTest = 0#In this experiment we are going to apply leave one out method on the MNIST datasets then we tune the hyper parameters of MulitiLayer perceptron classifier.\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        ## Splitting the data into train and test\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test  = y[train_index], y[test_index]\n",
    "        ##fit the data into the model\n",
    "        clf.fit(X_train,Y_train)\n",
    "        ##predicting the values on the fitted model using train data\n",
    "        predTrain = clf.predict((X_train))\n",
    "        #adding the accuracy\n",
    "        tempTrain = tempTrain + accuracy(Y_train,predTrain)\n",
    "        ##predict the values on the fitted model using test data\n",
    "        predTest = clf.predict((X_test))\n",
    "        #adding the accuracy\n",
    "        tempTest =  tempTest + accuracy(Y_test,predTest)\n",
    "    ##Calculating the train accuracy\n",
    "    train_accuracy.append(tempTrain*1.0/4)\n",
    "    ##Calculating the test accuracy\n",
    "    test_accuracy.append(tempTest*1.0/4)\n",
    "    print(\"(train,test) accuracy = \",tempTrain*1.0/4, tempTest*1.0/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Plotting the data\n",
    "xx = np.array(range(1,11))\n",
    "plt.bar(xx-0.2,train_accuracy,width=0.2)\n",
    "plt.bar(xx, test_accuracy,width=0.2)\n",
    "plt.legend([\"Train\",\"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
