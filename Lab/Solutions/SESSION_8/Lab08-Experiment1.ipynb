{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab08 Experiment 01\n",
    "\n",
    "## MLP and Backpropagation ##\n",
    "\n",
    "## Training a MLP\n",
    "\n",
    "Here, we will build a 3-layer neural network with one input layer, one hidden layer, and one output layer. The number of nodes in the input layer is determined by the dimensionality of our data. Similarly, the number of nodes in the output layer is determined by the number of classes we have.\n",
    "\n",
    "### How our network makes predictions\n",
    "\n",
    "Our network makes predictions using *forward propagation*, which is just a bunch of matrix multiplications and the application of the activation function(s) we defined above. If $x$ is the $N$-dimensional input to our network then we calculate our prediction $\\hat{y}$ (of lets say dimension $C$) as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 & = xW_1 + b_1 \\\\\n",
    "a_1 & = \\tanh(z_1) \\\\\n",
    "z_2 & = a_1W_2 + b_2 \\\\\n",
    "a_2 & = \\hat{y} = \\mathrm{softmax}(z_2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$z_i$ is the weighted sum of inputs of layer $i$ (bias included) and $a_i$ is the output of layer $i$ after applying the activation function. $W_1, b_1, W_2, b_2$ are  parameters of our network, which we need to learn from our training data. You can think of them as matrices transforming data between layers of the network. Looking at the matrix multiplications above we can figure out the dimensionality of these matrices. If we use 100 nodes for our hidden layer then $W_1 \\in \\mathbb{R}^{N\\times100}$, $b_1 \\in \\mathbb{R}^{100}$, $W_2 \\in \\mathbb{R}^{100\\times C}$, $b_2 \\in \\mathbb{R}^{C}$. Now you see why we have more parameters if we increase the size of the hidden layer.\n",
    "\n",
    "### Learning the Parameters/Backpropagation\n",
    "\n",
    "Learning the parameters for our network means finding parameters ($W_1, b_1, W_2, b_2$) that minimize the error on our training data. But how do we define the error? We call the function that measures our error the *loss function*. A common choice with the softmax output is the cross-entropy loss. If we have $N$ training examples and $C$ classes then the loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The formula looks complicated, but all it really does is sum over our training examples and add to the loss if we predicted the incorrect class. So, the further away $y$ (the correct labels) and $\\hat{y}$ (our predictions) are, the greater our loss will be. \n",
    "\n",
    "Remember that our goal is to find the parameters that minimize our loss function. We can use gradient descent to find its minimum. Here, we implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice but we are not going into that in this experiment.\n",
    "\n",
    "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. To calculate these gradients we use the famous *backpropagation algorithm*, which is a way to efficiently calculate the gradients starting from the output.\n",
    "\n",
    "Applying the backpropagation formula using chain rule we find the following:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_3 = \\frac{\\partial{L}}{\\partial{z_2}} = \\frac{\\partial{L}}{\\partial{a_2}}\\times\\frac{\\partial{a_2}}{\\partial{z_2}} = -(y - \\hat{y})\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $a_2$ is $\\hat{y}$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial{L}}{\\partial{W_2}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{W_2}} = a_1^T \\delta_3  \\\\\n",
    "& \\frac{\\partial{L}}{\\partial{b_2}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{b_2}} = \\delta_3\\\\\n",
    "& \\delta_2 = \\frac{\\partial{L}}{\\partial{z_1}} = \\frac{\\partial{L}}{\\partial{z_2}}\\times\\frac{\\partial{z_2}}{\\partial{a_1}}\\times\\frac{\\partial{a_1}}{\\partial{z_1}} = (1 - \\tanh^2z_1) \\circ \\delta_3W_2^T \\\\\n",
    "& \\frac{\\partial{L}}{\\partial{W_1}} = \\frac{\\partial{L}}{\\partial{z_1}}\\times\\frac{\\partial{z_1}}{\\partial{W_1}} = x^T \\delta_2\\\\\n",
    "& \\frac{\\partial{L}}{\\partial{b_1}} = \\frac{\\partial{L}}{\\partial{z_1}}\\times\\frac{\\partial{z_1}}{\\partial{b_1}} = \\delta_2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\delta_3 = $ derivative of cross-entropy loss with Softmax as Activation [We will not go into its derivation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Load MNIST dataset \n",
    "digits = datasets.load_digits(n_class=10)\n",
    "# Create our X and y data\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "print(X.shape, Y.shape)\n",
    "num_examples = X.shape[0]      ## training set size\n",
    "nn_input_dim = X.shape[1]      ## input layer dimensionality\n",
    "nn_output_dim = len(np.unique(Y))       ## output layer dimensionality\n",
    "\n",
    "params = {\n",
    "    \"lr\":1e-5,        ## learning_rate\n",
    "    \"max_iter\":1000,\n",
    "    \"h_dimn\":40,     ## hidden_layer_size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Writing helper functions for the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    hdim = params[\"h_dimn\"]\n",
    "    # Initialize the parameters to random values.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, hdim))\n",
    "    W2 = np.random.randn(hdim, nn_output_dim) / np.sqrt(hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return model\n",
    "\n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "def feedforward(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    probs = softmax(z2)\n",
    "    return a1, probs\n",
    "\n",
    "def backpropagation(model, x, y, a1, probs):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    delta3 = probs\n",
    "    delta3[range(y.shape[0]), y] -= 1\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "    dW1 = np.dot(x.T, delta2)\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    return dW2, db2, dW1, db1\n",
    "\n",
    "def calculate_loss(model, x, y):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # Forward propagation to calculate predictions\n",
    "    _, probs = feedforward(model, x)\n",
    "    \n",
    "    # Calculating the cross entropy loss\n",
    "    corect_logprobs = -np.log(probs[range(y.shape[0]), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    \n",
    "    return 1./y.shape[0] * data_loss\n",
    "\n",
    "def test(model, x, y):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation to calculate predictions\n",
    "    _, probs = feedforward(model, x)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    return np.count_nonzero(y==preds)/y.shape[0]\n",
    "\n",
    "def train(model, X_train, X_test, Y_train, Y_test, print_loss=True):\n",
    "    # Gradient descent. For each batch...\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    for i in range(0, params[\"max_iter\"]):\n",
    "\n",
    "        # Forward propagation\n",
    "        a1, probs = feedforward(model, X_train)\n",
    "\n",
    "        # Backpropagation\n",
    "        dW2, db2, dW1, db1 = backpropagation(model, X_train, Y_train, a1, probs)\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -params[\"lr\"] * dW1\n",
    "        b1 += -params[\"lr\"] * db1\n",
    "        W2 += -params[\"lr\"] * dW2\n",
    "        b2 += -params[\"lr\"] * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        if print_loss and i % 50 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model, X_train, Y_train)),\n",
    "                  \", Test accuracy:\", test(model, X_test, Y_test), \"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.492749 , Test accuracy: 0.13236929922135707 \n",
      "\n",
      "Loss after iteration 50: 1.712895 , Test accuracy: 0.45050055617352613 \n",
      "\n",
      "Loss after iteration 100: 1.311046 , Test accuracy: 0.6596218020022246 \n",
      "\n",
      "Loss after iteration 150: 1.043388 , Test accuracy: 0.7686318131256952 \n",
      "\n",
      "Loss after iteration 200: 0.858650 , Test accuracy: 0.8131256952169077 \n",
      "\n",
      "Loss after iteration 250: 0.725068 , Test accuracy: 0.8498331479421579 \n",
      "\n",
      "Loss after iteration 300: 0.625338 , Test accuracy: 0.8654060066740823 \n",
      "\n",
      "Loss after iteration 350: 0.548757 , Test accuracy: 0.882091212458287 \n",
      "\n",
      "Loss after iteration 400: 0.486890 , Test accuracy: 0.8921023359288098 \n",
      "\n",
      "Loss after iteration 450: 0.436110 , Test accuracy: 0.8987764182424917 \n",
      "\n",
      "Loss after iteration 500: 0.392825 , Test accuracy: 0.9087875417130145 \n",
      "\n",
      "Loss after iteration 550: 0.355599 , Test accuracy: 0.917686318131257 \n",
      "\n",
      "Loss after iteration 600: 0.324155 , Test accuracy: 0.9210233592880979 \n",
      "\n",
      "Loss after iteration 650: 0.297017 , Test accuracy: 0.9221357063403782 \n",
      "\n",
      "Loss after iteration 700: 0.273805 , Test accuracy: 0.9254727474972191 \n",
      "\n",
      "Loss after iteration 750: 0.253553 , Test accuracy: 0.92880978865406 \n",
      "\n",
      "Loss after iteration 800: 0.235535 , Test accuracy: 0.92880978865406 \n",
      "\n",
      "Loss after iteration 850: 0.219625 , Test accuracy: 0.9299221357063404 \n",
      "\n",
      "Loss after iteration 900: 0.205759 , Test accuracy: 0.932146829810901 \n",
      "\n",
      "Loss after iteration 950: 0.193485 , Test accuracy: 0.9354838709677419 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)\n",
    "\n",
    "model = train(model, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
