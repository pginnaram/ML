{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab08 Experiment 04\n",
    "\n",
    "## Regularization ##\n",
    "\n",
    "Regularization involves adding an extra term to the loss function, which penalizes certain parameter configurations.\n",
    "\n",
    "In principle, adding a regularization term to the loss will encourage smooth network mappings in a neural network (by penalizing large values of the parameters, which decreases the amount of nonlinearity that the network models).\n",
    "\n",
    "The regularization term in loss acts as a weight decay factor during back propagation and is used for controlling complexity of model (to prevent overfitting) by penalizing weights with large magnitude.\n",
    "\n",
    "The L1 regularization will shrink some parameters to zero. Hence some variables will not play any role in the model, L1 regression can be seen as a way to select features in a model. However, the model is not able to learn complex pattern with so few parameters remaining. Here, for each weight $w$ we add the term $\\lambda∣w∣$ to the objective.\n",
    "\n",
    "In this experiment, we are implementing L1 regularization in combination with L2 Regularization. The L2 regularization adds a penalty equal to the sum of the squared value of the coefficients. The L2 regularization will force the parameters to be relatively small, the bigger the penalization, the smaller (and the more robust) the coefficients are. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. That is, for every weight $w$ in the network, we add the term $\\lambda\\times(1/2)\\times w^2$ to the objective, where λ is the regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "#Load MNIST datset \n",
    "digits = datasets.load_digits(n_class=10)\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "print(X.shape, Y.shape)\n",
    "num_examples = X.shape[0]      ## training set size\n",
    "nn_input_dim = X.shape[1]      ## input layer dimensionality\n",
    "nn_output_dim = len(np.unique(Y))       ## output layer dimensionality\n",
    "\n",
    "params = {\n",
    "    \"lr\":0.0001,        ## learning_rate\n",
    "    \"max_iter\":500,\n",
    "    \"h_dimn\":50,     ## hidden_layer_size\n",
    "    \"regL1\":1,\n",
    "    \"regL2\":1,\n",
    "}\n",
    "print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "def build_model():\n",
    "    hdim = params[\"h_dimn\"]\n",
    "    # Initialize the parameters to random values.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.random.randn(1, hdim)\n",
    "    W2 = np.random.randn(hdim, nn_output_dim) / np.sqrt(hdim)\n",
    "    b2 = np.random.randn(1, nn_output_dim)\n",
    "\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return model\n",
    "\n",
    "def feedforward(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    probs = softmax(z2)\n",
    "    return a1, probs\n",
    "\n",
    "def backpropagation(model, x, y, a1, probs):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    delta3 = probs\n",
    "    delta3[range(y.shape[0]), y] -= 1\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "    dW1 = np.dot(x.T, delta2)\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "    return dW2, db2, dW1, db1\n",
    "\n",
    "def calculate_loss(model, params, x, y):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # Forward propagation to calculate predictions\n",
    "    _, probs = feedforward(model, x)\n",
    "    \n",
    "    # Calculating the cross entropy loss\n",
    "    corect_logprobs = -np.log(probs[range(y.shape[0]), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "#     data_loss = -sum(np.matmul(np.log(probs).T,y))\n",
    "    \n",
    "    # Add regulatization terms to loss  =  reg_factor*(1/2)*(||W||^2) + reg_factor*|W|\n",
    "    data_loss += params[\"regL2\"]/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    data_loss += params[\"regL1\"] * (np.linalg.norm(W1, ord=1) + np.linalg.norm(W2, ord=1))\n",
    "    \n",
    "    return 1./y.shape[0] * data_loss\n",
    "\n",
    "def test(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation to calculate predictions\n",
    "    _, probs = feedforward(model, x)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    return preds\n",
    "\n",
    "def train(model, X_train, X_test, Y_train, Y_test, verbose=True):\n",
    "    # Gradient descent. For each batch...\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    for i in range(0, params[\"max_iter\"]):\n",
    "\n",
    "        # Forward propagation\n",
    "        a1, probs = feedforward(model, X_train)\n",
    "\n",
    "        # Backpropagation\n",
    "        dW2, db2, dW1, db1 = backpropagation(model, X_train, Y_train, a1, probs)\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += params[\"regL2\"] * W2    ## = derivative of [[ reg_factor*(1/2)*(||W||^2) ]]\n",
    "        dW1 += params[\"regL2\"] * W1\n",
    "        dW1 += params[\"regL1\"]         ## derivative of[[  reg_factor*|W|  ]]\n",
    "        dW2 += params[\"regL1\"]\n",
    "        \n",
    "        # Gradient descent parameter update\n",
    "        W1 += -params[\"lr\"] * dW1\n",
    "        b1 += -params[\"lr\"] * db1\n",
    "        W2 += -params[\"lr\"] * dW2\n",
    "        b2 += -params[\"lr\"] * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        if verbose and i % 10 == 0:\n",
    "            preds = test(model, X_test)\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model, params, X_train, Y_train)),\n",
    "                  \", Test accuracy:\", np.count_nonzero(Y_test==preds)/Y_test.shape[0], \"\\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0001, 'regL1': 0, 'max_iter': 500, 'h_dimn': 50, 'regL2': 0}\n",
      "test accuracy 0.9518072289156626 \n",
      "\n",
      "{'lr': 0.0001, 'regL1': 0, 'max_iter': 500, 'h_dimn': 50, 'regL2': 0.1}\n",
      "test accuracy 0.9518072289156626 \n",
      "\n",
      "{'lr': 0.0001, 'regL1': 0, 'max_iter': 500, 'h_dimn': 50, 'regL2': 1}\n",
      "test accuracy 0.9527340129749768 \n",
      "\n",
      "{'lr': 0.0001, 'regL1': 0, 'max_iter': 500, 'h_dimn': 50, 'regL2': 10}\n",
      "test accuracy 0.9518072289156626 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.6)\n",
    "reg = [0, 0.1, 1, 10]\n",
    "params[\"regL1\"] = 0    ## L2 is better than L1 in practice\n",
    "for i in range(4):\n",
    "    params[\"regL2\"] = reg[i]\n",
    "    print(params)\n",
    "    model = build_model()\n",
    "    model = train(model, X_train, X_test, Y_train, Y_test, verbose=False)\n",
    "    preds = test(model, X_test)\n",
    "    #train.pop()[0]\n",
    "\n",
    "    test_acc = np.count_nonzero(Y_test==preds)/Y_test.shape[0]\n",
    "    preds = test(model, X_train)\n",
    "    train_acc = np.count_nonzero(Y_train==preds)/Y_train.shape[0]\n",
    "    print(\"test accuracy\", test_acc, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of regularization is not that visible for the above dataset because of the less number of parameters and no overfitting.\n",
    "\n",
    "Overfitting is a very common problem when the dataset is too small compared with the number of model parameters that need to be learned. This problem is thus particularly acute in deep neural networks. Therefore, we will check its effects on a larger network using a optimized MLP Classifier (provided by sklearn). The above exercise was mainly put to understand how we can optimize our model by adding regularizations in the objective function for a neural network. There are other forms of regularization (like dropout, etc) too but we will not go into those details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 784) (7000,)\n",
      "reg=  10  , acc =  0.20761904761904762\n",
      "reg=  1  , acc =  0.10071428571428571\n",
      "reg=  0.001  , acc =  0.10595238095238095\n",
      "reg=  0  , acc =  0.8645238095238095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, Y = mnist.data, mnist.target\n",
    "Y = Y.astype(int)\n",
    "X = X[::10,:]\n",
    "Y = Y[::10]\n",
    "print(X.shape, Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.6)\n",
    "\n",
    "reg = [10, 1, 0.001, 0]\n",
    "for i in range(4):\n",
    "    clf = MLPClassifier(hidden_layer_sizes = (350,120,50), solver = 'sgd', alpha=reg[i], max_iter=2000, shuffle = False)\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    test_acc = np.count_nonzero(Y_test==preds)/Y_test.shape[0]\n",
    "    print(\"reg= \", reg[i], \" , acc = \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
