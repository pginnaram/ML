{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 05\n",
    "### Experiment 1 - Part 4\n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "**Objectives:** Here we will use Stochastic Gradient Descent Method in which every next point is chosen randomly.\n",
    "\n",
    "**Expected Time:** This Experiment should take around 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Datasets/regr01.txt\", sep=\" \", header=None, names=['l', 't'])\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data['l'].values\n",
    "t = data['t'].values\n",
    "tsq = t * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic gradient descent (Single sample)**\n",
    "\n",
    "Instead of computing the sum of all gradients, stochastic gradient descent selects an observation uniformly at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = mx + c$\n",
    "\n",
    "$E$ = $(y_i - y)^2$\n",
    "\n",
    "$\\frac{\\partial E }{\\partial m}$ = $ -(y_i - (mx_i + c)) * x_i$\n",
    "\n",
    "$\\frac{\\partial E }{\\partial c}$ = $ -(y_i - (mx_i + c))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step(x, y, m, c, eta):\n",
    "    ycalc = m * x + c\n",
    "    error = (y - ycalc) ** 2\n",
    "    delta_m = -(y - ycalc) * x\n",
    "    delta_c = -(y - ycalc)\n",
    "    m = m - delta_m * eta\n",
    "    c = c - delta_c * eta\n",
    "    return m, c, error\n",
    "\n",
    "def one_loop_random(x, y, m, c, eta):\n",
    "    # Making random idx\n",
    "    random_idx = np.arange(len(y))\n",
    "    np.random.shuffle(random_idx)\n",
    "    # Training with random idx\n",
    "    for idx in random_idx:\n",
    "        m, c, e = next_step(x[idx], y[idx], m, c, eta)\n",
    "        #print(m, c, e)\n",
    "    return m,c,e\n",
    "\n",
    "def train_stochastic(x, y, m, c, eta, iterations=1000):\n",
    "    for iteration in range(iterations):\n",
    "        m, c, err = one_loop_random(x, y, m, c, eta)\n",
    "    return m, c, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init m, c\n",
    "m, c = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 1000 iterations, plotting after every 100 iterations:\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "for num in range(10):\n",
    "    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100)\n",
    "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
    "    y = m * l + c\n",
    "    ax.clear()\n",
    "    ax.plot(l, tsq, '.k')\n",
    "    ax.plot(l, y)\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Experiment with other lr values.**\n",
    "\n",
    "## Plotting error vs iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms, cs,errs = [], [], []\n",
    "m, c = 0, 0\n",
    "lr = 0.0001\n",
    "for times in range(100):\n",
    "    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100) # We will plot the value of for every 100 iterations\n",
    "    ms.append(m)\n",
    "    cs.append(c)\n",
    "    errs.append(error)\n",
    "epochs = range(0, 10000, 100)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, errs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Stochastic Gradient Descent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Is this better than sequential gradient descent and vanilla gradient descent?**\n",
    "\n",
    "Hint - check the error value at saturation, and time it takes to reach saturation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM\n",
    "\n",
    "Problem with Sequential Gradient Descent is it does not scale well - it makes the same calculation of gradient descent on each sample sequentially. So the time taken will increase linearly with the number of samples. Many datasets have samples in the range of millions. Hence, even though it gives good results, it is not ideal.\n",
    "\n",
    "We need a gradient descent formulation that gives the speed of vanilla gradient descent and the accuracy of sequential/stochastic gradient descent.\n",
    "\n",
    "Next up - **Minibatch Gradient Descent!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Last Error at saturation: 0.004\n",
    "def find_itr(epochs,errs):\n",
    "    for i in range(len(epochs)):\n",
    "        if(errs[i] <= 0.004):\n",
    "            return epochs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_itr(epochs,errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
