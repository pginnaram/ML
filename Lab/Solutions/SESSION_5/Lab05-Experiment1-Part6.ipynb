{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 05\n",
    "### Experiment 1 - Part 6\n",
    "# Effect of LR Decay\n",
    "\n",
    "**Objectives:** Here you will see how to vary the learning rate and observe the phenomenon of oscillation around the optimal value, and the effect of decreasing learning rate.\n",
    "\n",
    "**Expected Time:** This Experiment should take around 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Datasets/regr01.txt\", sep=\" \", header=None, names=['l', 't'])\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data['l'].values\n",
    "t = data['t'].values\n",
    "tsq = t * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla/Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, m, c, eta):\n",
    "    const = - 2.0/len(y)\n",
    "    ycalc = m * x + c\n",
    "    delta_m = const * sum(x * (y - ycalc))\n",
    "    delta_c = const * sum(y - ycalc)\n",
    "    m = m - delta_m * eta\n",
    "    c = c - delta_c * eta\n",
    "    error = sum((y - ycalc)**2)/len(y)\n",
    "    return m, c, error\n",
    "\n",
    "def train_on_all(x, y, m, c, eta, iterations=1000):\n",
    "    for steps in range(iterations):\n",
    "        m, c, err = train(x, y, m, c, eta)\n",
    "    return m, c, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of varying LR on error and final line\n",
    "\n",
    "Let us vary LR and find how the error decreases in each case, and how the final line looks, by training each case for the same nuber of iterations - 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\eta$ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save errors\n",
    "'''Checking the algo for different learning rates'''\n",
    "errs_1 = []\n",
    "m, c = 0, 0\n",
    "eta = 0.1\n",
    "for iteration in range(2000):\n",
    "    m, c, error = train(l, tsq, m, c, eta)\n",
    "    errs_1.append(error)\n",
    "\n",
    "# Save final line\n",
    "m_1, c_1 = m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\eta$ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_01 = []\n",
    "m, c = 0, 0\n",
    "eta = 0.01\n",
    "for iteration in range(2000):\n",
    "    m, c, error = train(l, tsq, m, c, eta)\n",
    "    errs_01.append(error)\n",
    "\n",
    "# Save final line\n",
    "m_01, c_01 = m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\eta$ = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_001 = []\n",
    "m, c = 0, 0\n",
    "eta = 0.001\n",
    "for iteration in range(2000):\n",
    "    m, c, error = train(l, tsq, m, c, eta) # We will plot the value of for every 100 iterations\n",
    "    errs_001.append(error)\n",
    "\n",
    "# Save final line\n",
    "m_001, c_001 = m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\eta$ = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_0001 = []\n",
    "m, c = 0, 0\n",
    "eta = 0.0001\n",
    "for iteration in range(2000):\n",
    "    m, c, error = train(l, tsq, m, c, eta) # We will plot the value of for every 100 iterations\n",
    "    errs_0001.append(error)\n",
    "\n",
    "# Save final line\n",
    "m_0001, c_0001 = m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of lines vs $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lines\n",
    "y_1 = m_1 * l + c_1\n",
    "y_01 = m_01 * l + c_01\n",
    "y_001 = m_001 * l + c_001\n",
    "y_0001 = m_0001 * l + c_0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(l, tsq, '.k')\n",
    "plt.plot(l, y_1, \"g\")\n",
    "plt.plot(l, y_01, \"r\")\n",
    "plt.plot(l, y_001, \"b\")\n",
    "plt.plot(l, y_0001, \"y\")\n",
    "plt.legend([\"l vs tsq\",\"eta = 0.1\",\"eta = 0.01\",\"eta = 0.001\",\"eta = 0.0001\"])\n",
    "plt.show()\n",
    "\n",
    "'''the plot shows lines for different learning rates'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that higher learning rates reach the best fit faster than lower learning rates (obviously)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of errors vs epochs for each $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(0,2000)\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.plot(epochs, errs_1, \"g\")\n",
    "plt.plot(epochs, errs_01,\"r\")\n",
    "plt.plot(epochs, errs_001,\"b\")\n",
    "plt.plot(epochs, errs_0001,\"y\")\n",
    "plt.legend([\"eta = 0.1\",\"eta = 0.01\",\"eta = 0.001\",\"eta = 0.0001\"])\n",
    "plt.show()\n",
    "\n",
    "'''plot for errors vs epochs of each learning rate '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With LR Decay\n",
    "\n",
    "In some cases, the learning rate might be too high to give good fitting lines. For example, let us train with constant LR of 0.8 and get the final line after 1000 iterations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\eta$ = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = []\n",
    "m, c = 0, 0\n",
    "eta = 0.8\n",
    "for times in range(1000):\n",
    "    m, c, error = train(l, tsq, m, c, eta)\n",
    "    errs.append(error)\n",
    "    \n",
    "m_normal, c_normal = m, c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the plot of error vs iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(errs)), errs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the error quickly goes to almost 0, but after some iterations blows up.\n",
    "\n",
    "Let us check the \"best fit\" line that is found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m_normal, c_normal, errs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m_normal * l + c_normal \n",
    "plt.plot(l, tsq, '.k')\n",
    "plt.plot(l,y,\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this is not ideal.\n",
    "\n",
    "This was a simple case where we can see the learning rate is too high. There might be cases where it is not so simple to identify this. Also, having a low learning rate is not good because training time would be too high!\n",
    "\n",
    "**Solution: Decay the learning rate.**\n",
    "\n",
    "Now let us train another model with decaying lr. But let us not decay lr below 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_decay = []\n",
    "m, c = 0, 0\n",
    "eta = 0.5\n",
    "decay_factor = 0.99\n",
    "for iteration in range(1000):\n",
    "    eta = max(0.0001, eta * decay_factor)\n",
    "    m, c, error = train(l, tsq, m, c, eta)\n",
    "    errs_decay.append(error)\n",
    "\n",
    "m_decay, c_decay = m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m_decay, c_decay, errs_decay[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(errs_decay)), errs_decay)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m_decay * l + c_decay \n",
    "plt.plot(l, tsq, '.k')\n",
    "plt.plot(l,y,\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, this is correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
