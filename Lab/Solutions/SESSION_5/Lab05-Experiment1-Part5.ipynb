{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 05\n",
    "### Experiment 1 - Part 5\n",
    "## Mini-batch Gradient Descent\n",
    "\n",
    "**Objectives:** In this Experiment we will use mini batch algorithm. In every iteration we use a set of 'm' training examples called batch to compute the gradient of the cost function. \n",
    "\n",
    "**Expected Time:** This Experiment should take around 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the file\n",
    "data = pd.read_csv(\"../Datasets/regr01.txt\", sep=\" \", header=None, names=['l', 't'])\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained already we will be plotting $ l $  vs $tsq $ and we will be fitting the line $ l $  vs $tsq $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data['l'].values\n",
    "t = data['t'].values\n",
    "tsq = t * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Mini-Batch Gradient Descent algorithm, rather than using  the complete data set, in every iteration we use a subset of training examples (called \"batch\") to compute the gradient of the cost function. \n",
    "\n",
    "Common mini-batch sizes range between 50 and 256, but can vary for different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one_batch() : we will be calculating the essenial parts of the Gradient Descent method:  \n",
    "\n",
    "$y = mx + c$\n",
    "        \n",
    "$E$ =$\\frac{1}{n}$   $\\sum_{i=1}^n (y_i - y)^2$\n",
    "\n",
    "$\\frac{\\partial E }{\\partial m}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -x_i(y_i - (mx_i + c))$\n",
    " \n",
    "$\\frac{\\partial E}{\\partial c}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -(y_i - (mx_i + c))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one_step() : We will be splitting our data into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here in mini-batch we go through all samples in a batch and then\n",
    "update at the end'''\n",
    "\n",
    "def train_one_batch(x, y, m, c, eta):\n",
    "    const = - 2.0/len(y)\n",
    "    ycalc = m * x + c\n",
    "    delta_m = const * sum(x * (y - ycalc))\n",
    "    delta_c = const * sum(y - ycalc)\n",
    "    m = m - delta_m * eta\n",
    "    c = c - delta_c * eta\n",
    "    error = sum((y - ycalc)**2)/len(y)\n",
    "    return m, c, error\n",
    "\n",
    "def train_batches(x, y, m, c, eta, batch_size):\n",
    "    # Making the batches\n",
    "    random_idx = np.arange(len(y))\n",
    "    np.random.shuffle(random_idx)\n",
    "    \n",
    "    # Train each batch\n",
    "    for batch in range(len(y)//batch_size):\n",
    "        batch_idx = random_idx[batch*batch_size:(batch+1)*batch_size]\n",
    "        batch_x = x[batch_idx]\n",
    "        batch_y = y[batch_idx]\n",
    "        m, c, err = train_one_batch(batch_x, batch_y, m, c, eta)\n",
    "    \n",
    "    return m, c, err\n",
    "\n",
    "def train_minibatch(x, y, m, c, eta, batch_size=10, iterations=1000):\n",
    "    for iteration in range(iterations):\n",
    "        m, c, err = train_batches(x, y, m, c, eta, batch_size)\n",
    "    return m, c, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init m, c\n",
    "m, c = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 1000 iterations, plotting after every 100 iterations:\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "for num in range(10):\n",
    "    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size=30, iterations=100)\n",
    "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
    "    y = m * l + c\n",
    "    ax.clear()\n",
    "    ax.plot(l, tsq, '.k')\n",
    "    ax.plot(l, y)\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Experiment with other batch_size values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us change Batch size to 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 1000 iterations, plotting after every 100 iterations:\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "for num in range(10):\n",
    "    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size=25, iterations=100)\n",
    "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
    "    y = m * l + c\n",
    "    ax.clear()\n",
    "    ax.plot(l, tsq, '.k')\n",
    "    ax.plot(l, y)\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Experiment with other lr values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us change the lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 1000 iterations, plotting after every 100 iterations:\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "lr = 0.1\n",
    "for num in range(10):\n",
    "    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size=25, iterations=100)\n",
    "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
    "    y = m * l + c\n",
    "    ax.clear()\n",
    "    ax.plot(l, tsq, '.k')\n",
    "    ax.plot(l, y)\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting error vs iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ms, cs,errs = [], [], []\n",
    "m, c = 0, 0\n",
    "lr = 0.001\n",
    "batch_size = 10\n",
    "for times in range(100):\n",
    "    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size, iterations=100) # We will plot the value of for every 100 iterations\n",
    "    ms.append(m)\n",
    "    cs.append(c)\n",
    "    errs.append(error)\n",
    "epoch = range(0, 10000, 100)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epoch, errs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Minibatch Gradient Descent\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''In the mini-batch gradient descent algo, the error saturation point can be observed around 0.006\n",
    "This shows that the mini-batch is better approach than sequential/stochastic (as they update for every individual sample)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Is this better than sequential gradient descent and vanilla gradient descent?**\n",
    "\n",
    "Hint - check the error value at saturation, and time it takes to reach saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Last Error at saturation: 0.006\n",
    "def find_itr(epochs,errs):\n",
    "    for i in range(len(epochs)):\n",
    "        if(errs[i] <= 0.006):\n",
    "            return epochs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_itr(epoch,errs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
