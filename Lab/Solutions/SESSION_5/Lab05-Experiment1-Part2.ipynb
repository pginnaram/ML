{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 05\n",
    "### Experiment 1 - Part 2\n",
    "## HANDS-ON TRAINING OF LINEAR REGRESSOR\n",
    "\n",
    "**Objectives:** We look at the sequence by which we can derive the best fit line. We will use the Gradient Descent and actually execute it in stages to understand how it works.\n",
    "\n",
    "**Expected Time:** This Experiment should take around 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stat\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the same data as before. We note that this is actual experimental data for the **Simple Pendulum** experiment; the length in cm and the period of oscillation in seconds are the two columns of data. So we will use the names **l** and **t** from now on. Recall that $L \\propto T^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Datasets/regr01.txt\", sep=\" \", header=None, names=['l', 't'])\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = data['l'].values\n",
    "t = data['t'].values\n",
    "tsq = t * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly summarise the essenial parts of the Gradient Descent method:\n",
    "\n",
    " $y = mx + c$\n",
    " \n",
    " $E$ =$\\frac{1}{n}$   $\\sum_{i=1}^n (y_i - y)^2$\n",
    " \n",
    " $\\frac{\\partial E }{\\partial m}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -x_i(y_i - (mx_i + c))$\n",
    " \n",
    " $\\frac{\\partial E}{\\partial c}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -(y_i - (mx_i + c))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, m, c, eta):\n",
    "    #w' = w + learning rate * change in gradient\n",
    "    const = - 2.0/len(y)\n",
    "    ycalc = m * x + c\n",
    "    delta_m = const * sum(x * (y - ycalc))\n",
    "    delta_c = const * sum(y - ycalc)\n",
    "    m = m - delta_m * eta\n",
    "    c = c - delta_c * eta\n",
    "    error = sum((y - ycalc)**2)/len(y)\n",
    "    return m, c, error\n",
    "\n",
    "def train_on_all(x, y, m, c, eta, iterations=1000):\n",
    "    for steps in range(iterations):\n",
    "        m, c, err = train(x, y, m, c, eta)\n",
    "    return m, c, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN\n",
    "\n",
    "## Let us visualize the training:\n",
    "### $\\eta$ = 0.01\n",
    "\n",
    "Training for 1000 iterations, plotting after every 100 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init m, c\n",
    "m, c = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 1000 iterations, plotting after every 100 iterations:\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "for num in range(10):\n",
    "    m, c, error = train_on_all(l, tsq, m, c, lr, iterations=100)\n",
    "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
    "    y = m * l + c\n",
    "    ax.clear()\n",
    "    ax.plot(l, tsq, '.k')\n",
    "    ax.plot(l, y)\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this is not enough.\n",
    "\n",
    "Let us train for a 1000 more iterations, with a greater learning rate:\n",
    "\n",
    "### $\\eta$ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 1000 iterations, plotting after every 100 iterations:\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "for num in range(10):\n",
    "    m, c, error = train_on_all(l, tsq, m, c, lr, iterations=100)\n",
    "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
    "    y = m * l + c\n",
    "    ax.clear()\n",
    "    ax.plot(l, tsq, '.k')\n",
    "    ax.plot(l, y)\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us continue with the same learning rate, and train for 1000 more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 1000 iterations, plotting after every 100 iterations:\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "for num in range(10):\n",
    "    m, c, error = train_on_all(l, tsq, m, c, lr, iterations=100)\n",
    "    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n",
    "    y = m * l + c\n",
    "    ax.clear()\n",
    "    ax.plot(l, tsq, '.k')\n",
    "    ax.plot(l, y)\n",
    "    fig.canvas.draw()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems correct.\n",
    "\n",
    "### Comparison with standard library\n",
    "\n",
    "Let us compare the values we found with the values found by the standard scipy library function \"stats\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From our Gradient Descent   m = {0:.06} c = {1:.06}\".format(m, c))\n",
    "\n",
    "msp, csp, _, _, _ = stat.linregress(l,tsq)\n",
    "print(\"From scipy.stats.linregress m = {0:.06} c = {1:.06}\".format(msp, csp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough!\n",
    "\n",
    "## Plotting error vs iterations\n",
    "\n",
    "So far we have seen how the Gradient Descent works by looking at the fit of the regression line. Let us change perspectives and plot the error at various stages. This just shows that the process is converging and gives us a feel for the rate at which it is converging.\n",
    "\n",
    "$E = \\frac{1}{n} ∑_{i=1}^n(y_i−y)^2$\n",
    "\n",
    "$ = \\frac{1}{n} ∑_{i=1}^n(y_i - mx_i - c)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms, cs,errs = [], [], []\n",
    "m, c = 0, 0\n",
    "eta = 0.001\n",
    "for times in range(200):\n",
    "    m, c, error = train_on_all(l, tsq, m, c, eta, iterations=100) # We will plot the value of for every 100 iterations\n",
    "    ms.append(m)\n",
    "    cs.append(c)\n",
    "    errs.append(error)\n",
    "epochs = range(0, 20000,100)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(epochs, errs)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Vanilla Gradient Descent\")\n",
    "plt.show()\n",
    "\n",
    "'''Error saturation is where the points in graph tends to a constant error value respective to iterations (on x-axis)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the error at saturation is around 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error vs m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the error as a function of **m** and **c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(x,y,m,c):\n",
    "    ycalc = m * x + c\n",
    "    error = sum((y - ycalc)**2) / len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The graph is projected which shows the relationship between error vs (m,c) - in 3D space'''\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ms1 = np.arange(-20, 20, 0.1)\n",
    "cs1 = np.arange(-20, 20, 0.1)\n",
    "X, Y = np.meshgrid(ms1, cs1)\n",
    "err = []\n",
    "for i in range(len(ms1)):\n",
    "    for j in range(len(cs1)):\n",
    "        err.append(error(l,tsq,ms1[i],cs1[j]))\n",
    "err = np.array(err)\n",
    "Z = np.reshape(err,(-1,len(ms1)))\n",
    "print(X.shape, Y.shape, Z.shape)\n",
    "ax.plot_surface(X, Y, Z) \n",
    "ax.set_xlabel('m')\n",
    "ax.set_ylabel('c')\n",
    "ax.set_zlabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
