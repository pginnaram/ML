{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab03A Experiment 01\n",
    "\n",
    "## Naive Bayes ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "In this experiment, we will use the Pima Indians Diabetes Data Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "data = pd.read_csv(\"pima-indians-diabetes.csv\",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some data samples from the dataset\n",
    "print('Pima Indians Diabetes Data Set\\n')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8th column is the class label\n",
    "print('\\n\\n\\nStats for the 7 features over the dataset and the 2 classes {8th column}{diabetic/not-diabetic}\\n')\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the data into train and test data, retaining 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_RATIO = 0.8        # 80% training data\n",
    "picker = list(range(data.shape[0]))        # get all indices as a list\n",
    "## sometimes the data is arranged classwise and not randomly\n",
    "## therefore we shuffle the indices\n",
    "random.shuffle(picker)\n",
    "trainMax = int(data.shape[0] * TRAIN_TEST_RATIO)\n",
    "\n",
    "train_features = []\n",
    "test_features = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for pick in picker[:trainMax]:\n",
    "    train_features.append(data.values[pick][:-1])\n",
    "    train_labels.append(int(data.values[pick][-1]))\n",
    "for pick in picker[trainMax:]:\n",
    "    test_features.append(data.values[pick][:-1])\n",
    "    test_labels.append(int(data.values[pick][-1]))\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "test_features = np.array(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values[pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.shape, len(train_labels), test_features.shape, len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Diabetes\n",
    "\n",
    "We have features and labels present in the dataset. Now we need to learn naive bayes classifier for this dataset.\n",
    "\n",
    "$$ P(y|X) = \\frac{P(X|y)*P(y)}{P(X)} \\propto (\\Pi_{i}P(X_{i}|y))*P(y) $$\n",
    "\n",
    "Here, 'y' represents a class, $ X_{i} $ represents the $ i^{th} $ feature and X is the set of all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate Prior $P(Y)$\n",
    "\n",
    "The formula for prior has been taught in class. This is also called the class probability. $P(Y)$ or $P(Y = y)$ is the fraction of the elements present in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique classes & corresponding number of elements belonging to each class\n",
    "classes, counts = np.unique(train_labels, return_counts=True)\n",
    "print(classes)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I assume my classes are from 0 ... N for some N (Here, we have just 2 classes)\n",
    "num_classes = len(classes)\n",
    "num_feats = train_features.shape[1]  #total number of features\n",
    "total_samples = len(train_labels)    #total number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Find the prior probability of each class as the list `prior`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior for any class = {number of samples belonging to that class/ total_samples}\n",
    "prior = ?????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Posterior Distribution $P(X| y)$ or $\\Pi_{i}P(X_{i}| y)$ from the data\n",
    "\n",
    "This is a little complicated. For each feature $X_{i}$, we assume that they are uncorrelated (Naive Bayes Assumption) and thus we can write,\n",
    "\n",
    "$$ P(X| y) = \\Pi_{i} P(X_{i} | y) $$\n",
    "\n",
    "We wish to estimate $P(X_{i} | y)$ for each $X_{i}$. Thus for samples corresponding to each class label $y$, we need to estimate the probability distribution. Here, however, our features are real values. So we will assume the distribution to be gaussian and estimate the parameters corresponding to each feature using all training samples per class. Thus, we need to calculate mean (say $ m_{y, i} $) and standard deviation (say $ v_{y, i} $) for each class $y$ and each feature $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the mean and variance per feature dimension here \n",
    "### from the training set from samples belonging to each class label.\n",
    "\n",
    "### Hint: Mean is the just the sum of a vector divided by the length of vector\n",
    "### Hint: Variance is the square of standard deviation.\n",
    "\n",
    "means = np.zeros((num_feats, num_classes)) # every feature, for each class\n",
    "stddev = np.zeros((num_feats, num_classes)) # every feature, for each class\n",
    "\n",
    "# For each class\n",
    "for y in classes: # selecting a class 'y'\n",
    "    pts = train_features[np.where( train_labels == y )[0], :]    # get all samples belonging to 'y'\n",
    "    # For each feature\n",
    "    for i in range(num_feats):\n",
    "        means[i, y] = np.mean(pts[:, i])\n",
    "        stddev[i, y] = np.std(pts[:, i])\n",
    "\n",
    "### This completes the training phase\n",
    "### We know have estimated both the prior probability and the posterior distributions from our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine prior and posterior to classify points.\n",
    "\n",
    "Now, given the mean 'm' and standard deviation 'v', the posterier probability is estimated by the normal distribution (also called gaussian distribution) using:\n",
    "\n",
    "$$ P(X_{i}| y) = \\frac{1}{\\sqrt{2\\pi v_{y, i}^{2}}} exp( - \\frac{ (X_{i} - m_{y, i})^{2} }{2v_{y, i}^{2}} ) $$ \n",
    "\n",
    "\n",
    "Use the formula given above and the parameters computed earlier, we predict the likelihood of every sample in a test set to belong to each class. $\\Pi$ stands for multiplication of each $P(X_{i}|y)$ ($i$ is the $i^{th}$ feature).\n",
    "\n",
    "Note: One may use other kinds of distributions too.\n",
    "\n",
    "### Exercise 2: Complete the Gaussian function based on the above equation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, m, v):\n",
    "    g = ?????????\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Find the likelihood for each class 'y', once you have $P(X_{i}|y)$ from Exercise 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(point, means, stddev):\n",
    "    \n",
    "    feat_prob = np.zeros((num_feats, num_classes))\n",
    "    for y in classes:\n",
    "        for i in range(num_feats):\n",
    "            feat_prob[i, y] = gaussian(point[i], means[i, y], stddev[i, y]) # get the probability\n",
    "    \n",
    "    likelihood = np.zeros((num_classes, 1)) # likelihood for each class 'y'\n",
    "    for y in classes:\n",
    "        ## your code here for Exercise 3\n",
    "        likelihood[y] = ?????????\n",
    "    \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict using Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "# For each test sample\n",
    "for i in range(len(test_labels)):\n",
    "    \n",
    "    # Get its likelihood of belong to either class\n",
    "    likelihood = get_likelihood(test_features[i, :], means, stddev)\n",
    "    \n",
    "    # Calculate the approximate posterior = likelihood * prior\n",
    "    approx_posterior = [ np.asscalar(x*y) for x,y in zip(likelihood, prior) ]\n",
    "    #approx because of missing P(X) (constant) in the denominator\n",
    "    \n",
    "    # Make the prediction as that class with the maximum approximate posterior\n",
    "    prediction = np.argmax(approx_posterior)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\")\n",
    "print(np.mean([x == y for x, y in zip(predictions, test_labels)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 4: Experiment with different size of train/test split to see the effect it has on classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions #\n",
    "## Exercise 1##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = np.array([ x*1.0/total_samples for x in counts ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, m, v):\n",
    "    g = np.sqrt(1.0/2*np.pi*v*v)*np.exp( -1.0*(((x - m)/v)**2) )\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in classes:\n",
    "    # Take the product of all the feature likelihoods of the class considered\n",
    "    likelihood[y] = np.prod(feat_prob[np.nonzero(feat_prob), y]) # mutliply for each feature 'Xi'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
