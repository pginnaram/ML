{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI & ML\n",
    "## Session 05\n",
    "### Experiment 2 - Part 1\n",
    "\n",
    "**Objectives:** Plot a Quadratic Equation representing an error function and see how to arrive at the minima in the plot.\n",
    "\n",
    "**Expected Time:** This Experiment should take around 20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are assuming the error  function = $ w^2 $ + $ 2w $ + $ 2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(w):\n",
    "    return (w**2) + (2*w) + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = range(-10,10)\n",
    "err = []\n",
    "for i in w:\n",
    "    err.append(error(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the error function is $ 2w $ + $ 2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w):\n",
    "    return 2*w + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a function for gradient descent that can be reused:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(w, eta):\n",
    "    return eta*gradient(w)\n",
    "\n",
    "def gradient_descent(eta, w, nb_of_iterations):\n",
    "    w_err = [np.array([w, error(w)])] # List to store the w, error values\n",
    "    for i in range(nb_of_iterations):\n",
    "        dw = delta(w, eta)  # Get the delta w update\n",
    "        w = w - dw  # Update the current w value\n",
    "        w_err.append(np.array([w, error(w)]))  # Add w, error to list\n",
    "    return np.array(w_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ eta $ = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate\n",
    "eta = 0.2\n",
    "\n",
    "#Set the initial parameter\n",
    "w = 5\n",
    "\n",
    "# number of gradient descent updates\n",
    "nb_of_iterations = 20\n",
    "\n",
    "w_err_02 = gradient_descent(eta, w, nb_of_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $eta $ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate\n",
    "eta = 0.5\n",
    "\n",
    "#Set the initial parameter\n",
    "w = 5\n",
    "\n",
    "# number of gradient descent updates\n",
    "nb_of_iterations = 20\n",
    "\n",
    "w_err_05 = gradient_descent(eta, w, nb_of_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $eta$ = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate\n",
    "eta = 0.7\n",
    "\n",
    "#Set the initial parameter\n",
    "w = 5\n",
    "\n",
    "# number of gradient descent updates\n",
    "nb_of_iterations = 20\n",
    "\n",
    "w_err_07 = gradient_descent(eta, w, nb_of_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final w, and cost\n",
    "for i in range(0, len(w_err_07)):\n",
    "    print('w({}): {:.4f} \\t cost: {:.4f}'.format(i, w_err_07[i][0], w_err_07[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let us see how to arrive at local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 10))\n",
    "plt.grid(True)\n",
    "plt.subplot(311)\n",
    "plt.plot(x, err)\n",
    "plt.plot(w_err_02[:,0], w_err_02[:,1],\"o\")\n",
    "plt.title([\"x vs m\",\"eta = 0.2\"])\n",
    "n = range(1, len(w_err_02[:,0]))\n",
    "for i, txt in enumerate(n):\n",
    "    plt.annotate(txt, (w_err_02[:,0][i], w_err_02[:,1][i]))\n",
    "plt.subplot(312)\n",
    "plt.plot(x, err)\n",
    "plt.plot(w_err_05[:,0], w_err_05[:,1],\"o\")\n",
    "plt.title([\"x vs m\",\"eta = 0.5\"])\n",
    "n = range(1, len(w_err_05[:,0]))\n",
    "for i, txt in enumerate(n):\n",
    "    plt.annotate(txt, (w_err_05[:,0][i], w_err_05[:,1][i]))\n",
    "plt.subplot(313)\n",
    "plt.plot(x, err)\n",
    "plt.plot(w_err_07[:,0], w_err_07[:,1],\"o\")\n",
    "plt.title([\"x vs m\",\"eta = 0.7\"])\n",
    "n = range(1, len(w_err_02[:,0]))\n",
    "for i, txt in enumerate(n):\n",
    "    plt.annotate(txt, (w_err_07[:,0][i], w_err_07[:,1][i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excerise : ** Observe how we reached to local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
