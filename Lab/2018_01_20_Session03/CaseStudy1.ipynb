{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1: Twitter Sentiment Analysis\n",
    "\n",
    "We have [Twitter Dataset](https://www.kaggle.com/c/twitter-sentiment-analysis2/data). We have to convert given tweets into features which can be used for sentiment classification(Positive and Negative Tweets). Every tweet can be classified as having either a positive or negative sentiment. Example of few tweets are:\n",
    "\n",
    "**Few Positive Tweets: **\n",
    "1.  @Msdebramaye I heard about that contest! Congrats girl!!\n",
    "2. UNC!!! NCAA Champs!! Franklin St.: I WAS THERE!! WILD AND CRAZY!!!!!! Nothing like it...EVER http://tinyurl.com/49955t3\n",
    "\n",
    "**Few Negative Tweets:**\n",
    "1. no more taking Irish car bombs with strange Australian women who can drink like rockstars...my head hurts.\n",
    "2. Just had some bloodwork done. My arm hurts\n",
    "\n",
    "We have 100,000 tweets for training and  300,000 tweets for testing. The Ground truth is 1 for positive tweet and 0 for negative tweet. Let's try to make a sentiment Analyzer using this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "import pandas as pd\n",
    "dataFrame = pd.read_csv(\"train.csv\",encoding='latin1')\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into array\n",
    "data = dataFrame.values\n",
    "n = dataFrame.shape[0] ## n is number of tweets\n",
    "print(n)\n",
    "\n",
    "##Stored labels and tweets in separate arrays for train data\n",
    "labels = data[:,1]\n",
    "tweets = data[:,2]\n",
    "print(labels.shape)\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Modify the tweets such that the irrelevant words and characters are removed. To this end apply the following preprocessing.\n",
    "1. **Case** Convert the tweets to lower case.\n",
    "2. **URLs** We don't intend to follow the (short) urls and determine the content of the site, so we can eliminate all of these URLs via regular expression matching or replace it with URL.\n",
    "3. **Username** We can eliminate \"$@$username\" via regex matching or replace it with AT\\_USER\n",
    "4. **hashtag** hash tags can give us some useful information, so replace them with the exact same word without the hash. E.g. \\#nike replaced with 'nike'.\n",
    "5. **Whitespace** Replace multiple whitespaces with a single whitespace.\n",
    "6. **Stop words** a, is, the, with etc. The full list of stop words can be found at Stop Word List. These words don't indicate any sentiment and can be removed.\n",
    "7. **Repeated letters** If you look at the tweets, sometimes people repeat letters to stress the emotion. E.g. hunggrryyy, huuuuuuungry for 'hungry'. We can look for 2 or more repetitive letters in words and replace them by 2 of the same.\n",
    "8. **Punctuation** Remove punctuation such as comma, single/double quote, question marks at the start and end of each word. E.g. beautiful!!!!!! replaced with beautiful\n",
    "9. **Non-alpha Words**  Remove all those words which don't start with an alphabet. E.g. 15th, 5.34am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess the tweets\n",
    "\n",
    "## import regex\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#start process_tweet\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    tweet = tweet.strip('.,')\n",
    "    return tweet\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    tweets[i] = processTweet(tweets[i])\n",
    "\n",
    "print(tweets[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Do further preprocessing to calculate count for number of positve words and number of negative words corresponding to each tweet. You can use [Positive_words.txt](https://drive.google.com/drive/folders/1TnJCyn4LiS6InT35skvCbbBrp37AGYc) and [Negative words.txt](https://drive.google.com/drive/folders/1TnJCyn4LiS6InT35skvCbbBrp37AGYcT}{negative\\_words.txt) which contain positive words and negative words respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "#start getStopWordList\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    for stopWord in open(stopWordListFileName, 'r'):\n",
    "        stopWords.append(stopWord)\n",
    "    return stopWords\n",
    "#end\n",
    "\n",
    "#start getfeatureVector\n",
    "def getFeatureVector(tweet):\n",
    "    featureVector = []\n",
    "    words = tweet.split()\n",
    "    PUNCTUATIONS = '\\'\"?!,.;:'    \n",
    "    for w in words:\n",
    "        # strip punctuation\n",
    "        w = w.strip(PUNCTUATIONS)\n",
    "        # check if the word starts with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    " \n",
    "        #ignore if it is a stop word\n",
    "        \n",
    "        if w in stopWords or val is None:\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector\n",
    "\n",
    "def getwordcount(words, count):    \n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    neutral_count = 0\n",
    "    \n",
    "    total = []\n",
    "    #print words\n",
    "    \n",
    "    for w in words:        \n",
    "        if w in positive_words:\n",
    "            positive_count += 1\n",
    "        elif w in negative_words:\n",
    "            negative_count += 1\n",
    "        else:\n",
    "            neutral_count += 1\n",
    "            \n",
    "    total.append(positive_count)\n",
    "    total.append(negative_count)\n",
    "    total.append(neutral_count)\n",
    "    total.append(labels[count])\n",
    "    return total\n",
    "    \n",
    "tweets_modified = []\n",
    "count = 0\n",
    "\n",
    "stopWords = getStopWordList('stopwords.txt')\n",
    "positive_words = pd.read_csv('positive-words.txt').values\n",
    "negative_words=  pd.read_csv('negative-words.txt').values\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    print(i)\n",
    "    print(tweets)\n",
    "    featureVector = getFeatureVector(tweets[i])\n",
    "    print(featureVector)\n",
    "    tweets_modified.append(getwordcount(featureVector,count))\n",
    "   \n",
    "    count += 1\n",
    "    #line = fp.readline()\n",
    "   # line1=sentiments.readline()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.asarray(tweets_modified)\n",
    "print (x.shape)\n",
    "print (x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the graph use features as positive count and negative count of each tweet. Also plot the garph by scaling the features and normalizing the features respecively. You need to plot total 3 graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using features as probabilities\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1, figsize=(20,10))\n",
    "\n",
    "colors = [\"red\",\"yellow\"]\n",
    "plt.scatter(x[:,0]/np.sum(x, 1, np.float), x[:,1]/np.sum(x, 1, np.float), c = colors, s=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Load the file test.csv and preprocess as above. Calculate the accuracy on test data using Linear classifier, or KNN classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = x[:70000,:2]\n",
    "train_Y = x[:70000,3]\n",
    "test_X = x[70000:,:2]\n",
    "test_Y = x[70000:,3]\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear classifier\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(train_X,train_Y)\n",
    "pred_label = (clf.predict(test_X))\n",
    "print(pred_label)\n",
    "correct = np.sum(abs(test_Y-pred_label))\n",
    "print(correct)\n",
    "accuracy = (correct/np.float(len(test_Y)))*100.0\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
