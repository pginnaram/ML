{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of AI/ML by IIIT-Hyderabad & Talent Sprint\n",
    "# Lab04 Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you will be working with very large datasets, so **kindly do not try to copy, move or download the datasets folder**. This will result in a drastic load on the network along with high utilization of your servers and slow down of everyones work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to introduce concepts in the text classification pipeline. We will briefly go over the powerful notions of bag of words, PCA and word2vec. \n",
    "\n",
    "In this experiment we will be exploring the task of news classification, for which we use the 20 news classification dataset: http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "To get a sense of our data, let us first start by counting the frequencies of the target classes in our news articles in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3 install gensim    #required library for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset = pickle.load(open('../Datasets/newsgroups.pkl','rb'))\n",
    "print(dataset.keys())\n",
    "\n",
    "# Print frequencies of dataset\n",
    "print(\"Class : count\")\n",
    "print(\"--------------\")\n",
    "number_of_documents = 0\n",
    "for key in dataset:\n",
    "    print(key, ':', len(dataset[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us split our dataset which consists of 1000 samples per class, into training and test sets. We use 950 samples from each class in the training set, and the remaining 50 in the test set. \n",
    "\n",
    "As a mental exercise you should try reasoning about why is it important to ensure a nearly equal distribution of classes in your training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = {}\n",
    "test_set = {}\n",
    "\n",
    "# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n",
    "for key in dataset:\n",
    "    dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in dataset[key]]\n",
    "    \n",
    "# Break dataset into 95-5 split for training and testing\n",
    "n_train = 0\n",
    "n_test = 0\n",
    "for k in dataset:\n",
    "    split = int(0.95*len(dataset[k]))\n",
    "    train_set[k] = dataset[k][0:split]\n",
    "    test_set[k] = dataset[k][split:-1]\n",
    "    n_train += len(train_set[k])\n",
    "    n_test += len(test_set[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bag-of-Words\n",
    "\n",
    "Let us begin our journey into text classification with one of the simplest but most commonly used feature representations for news documents - Bag-of-Words.\n",
    "\n",
    "As you might have realized, machine learning algorithms need good feature representations of different inputs.  Concretely, we would like to represent each news article $D$ in terms of a feature vector $V$, which can be used for classification. Feature vector $V$ is made up of the number of occurences of each word in the vocabulary.\n",
    "\n",
    "Let us begin by counting the number of occurences of every word in the news documents in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try understanding the kind of words that appear frequently, and those that occur rarely. We now count the frequencies of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to store frequencies of words.\n",
    "# Key:Value === Word:Count\n",
    "frequency = defaultdict(int)\n",
    "    \n",
    "for key in train_set:\n",
    "    for f in train_set[key]:\n",
    "        \n",
    "        # Find all words which consist only of capital and lowercase characters and are between length of 2-9.\n",
    "        # We ignore all special characters such as !.$ and words containing numbers\n",
    "        words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n",
    "    \n",
    "        for word in words:\n",
    "            frequency[word] += 1\n",
    "\n",
    "sorted_words = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Top-10 most frequent words:\")\n",
    "for word in sorted_words[:10]:\n",
    "    print(word)\n",
    "\n",
    "print('----------------------------')\n",
    "print(\"10 least frequent words:\")\n",
    "for word in sorted_words[-10:-1]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we attempt to plot a histogram of the counts of various words in descending order. \n",
    "\n",
    "Could you comment about the relationship between the frequency of the most frequent word to the second frequent word? \n",
    "And what about the third most frequent word?\n",
    "\n",
    "(Hint - Check the relative frequencies of the first, second and third most frequent words)\n",
    "\n",
    "(After answering, you can visit https://en.wikipedia.org/wiki/Zipf%27s_law for further Reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(20,10)\n",
    "\n",
    "plt.bar(range(len(sorted_words[:100])), [v for k, v in sorted_words[:100]] , align='center')\n",
    "plt.xticks(range(len(sorted_words[:100])), [k for k, v in sorted_words[:100]])\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Pre-processing to remove most and least frequent words\n",
    "\n",
    "We can see that different words appear with different frequencies.\n",
    "\n",
    "The most common words appear in almost all documents. Hence, for a classification task, having information about those words' frequencies does not mater much since they appear frequently in every type of document. To get a good feature representation, we eliminate them since they do not add too much value.\n",
    "\n",
    "Additionally, notice how the least frequent words appear so rarely that they might not be useful either.\n",
    "\n",
    "Let us pre-process our news articles now to remove the most frequent and least frequent words by thresholding their counts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words = defaultdict(int)\n",
    "\n",
    "print('Number of words before preprocessing:', len(sorted_words))\n",
    "\n",
    "# Ignore the 25 most frequent words, and the words which appear less than 100 times\n",
    "ignore_most_frequent = 25\n",
    "freq_thresh = 100\n",
    "feature_number = 0\n",
    "for word, word_frequency in sorted_words[ignore_most_frequent:]:\n",
    "    if word_frequency > freq_thresh:\n",
    "        valid_words[word] = feature_number\n",
    "        feature_number += 1\n",
    "        \n",
    "print('Number of words after preprocessing:', len(valid_words))\n",
    "\n",
    "word_vector_size = len(valid_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Bag-of-Words representation\n",
    "\n",
    "The simplest way to represent a document $D$ as a vector $V$ would be to now count the relevant words in the document. \n",
    "\n",
    "For each document, make a vector of the count of each of the words in the vocabulary (excluding the words removed in the previous step - the \"stopwords\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_BoW(dataset, number_of_documents):\n",
    "    bow_representation = np.zeros((number_of_documents, word_vector_size))\n",
    "    labels = np.zeros((number_of_documents, 1))\n",
    "    \n",
    "    i = 0\n",
    "    for label, class_name in enumerate(dataset):\n",
    "        \n",
    "        # For each file\n",
    "        for f in dataset[class_name]:\n",
    "            \n",
    "            # Read all text in file\n",
    "            text = ' '.join(f).split(' ')\n",
    "            \n",
    "            # For each word\n",
    "            for word in text:\n",
    "                if word in valid_words:\n",
    "                    bow_representation[i, valid_words[word]] += 1\n",
    "            \n",
    "            # Label of document\n",
    "            labels[i] = label\n",
    "            \n",
    "            # Increment document counter\n",
    "            i += 1\n",
    "    \n",
    "    return bow_representation, labels\n",
    "\n",
    "# Convert the dataset into their bag of words representation treating train and test separately\n",
    "train_bow_set, train_bow_labels = convert_to_BoW(train_set, n_train)\n",
    "test_bow_set, test_bow_labels = convert_to_BoW(test_set, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Document classification using Bag-of-Words\n",
    "\n",
    "For the test documents, use your favorite distance metric (Cosine, Eucilidean, etc.) to find similar news articles from your training set and classify using kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "\n",
    "# Optimized K-NN:- This does the same thing as you've learned but in an optimized manner\n",
    "def dist(train_features, given_feature):\n",
    "    squared_difference = (train_features - given_feature)**2\n",
    "    distances = np.sqrt(np.sum(squared_difference, axis = 1))\n",
    "    return distances\n",
    "\n",
    "''' \n",
    " Optimized K-NN code. This code is the same as what you've already seen, but trades off memory efficency \n",
    " for computational efficency. \n",
    "'''\n",
    "def kNN(k, train_features, train_labels, given_feature):\n",
    "    distances = []\n",
    "    \n",
    "    n = train_features.shape[0]\n",
    "    \n",
    "    # np.tile function repeats the given_feature n times.\n",
    "    given_feature = np.tile(given_feature, (n, 1))\n",
    "    \n",
    "    # Compute distance\n",
    "    distances = dist(train_features, given_feature)\n",
    "    sort_neighbors = np.argsort(distances)\n",
    "    return np.concatenate((distances[sort_neighbors][:k].reshape(-1, 1), train_labels[sort_neighbors][:k].reshape(-1, 1)), axis = 1)\n",
    "\n",
    "def kNN_classify(k, train_features, train_labels, given_feature):\n",
    "    tally = collections.Counter()\n",
    "    tally.update(str(int(nn[1])) for nn in kNN(k, train_features, train_labels, given_feature))\n",
    "    return int(tally.most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, using 3 nearest neighbours, the $0^{th}$ test document is classified as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN_classify(3, train_bow_set, train_bow_labels, test_bow_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy for the bag-of-words features on the full test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i, given_feature in enumerate(test_bow_set):\n",
    "    print(\"Progress: {0:.04f}\".format((i+1)/len(test_bow_set)), end=\"\\r\")\n",
    "    predicted_class = kNN_classify(3, train_bow_set, train_bow_labels, given_feature)\n",
    "    if predicted_class == int(test_bow_labels[i]):\n",
    "        accuracy += 1\n",
    "print(accuracy / len(test_bow_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw a naive way to represent words as dense vectors which can leverage the semantics of the words.\n",
    "\n",
    "The problem with count-based word representations is that they are costly in terms of memory to compute large co-occurrence matrices. Let us see another method to find representations of words without explicitly counting words.\n",
    "\n",
    "Here, we aim to predict the next word given the context in which the word appears. (For example, given the last $n$ words, predict the next word). A very smart way to do this is by using a feature representation called \"Word2Vec\" with transforms each word into 300-dimensional vectors.\n",
    "\n",
    "Link to pretrained 300 dimensional word2vec: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Visualizations\n",
    "\n",
    "Before we go to the actual 300 dimensional vectors, let's try to explore some of the more intriguing properties of word2vec.\n",
    "\n",
    "You have been provided with a sample of word vectors. **We have reduced the dimensionality of the 300-dimensional vectors to 2 dimensions, so that we can plot them in matplotlib.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_values(values, labels, figsize = (8,4), c = []):\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=figsize) \n",
    "    for i in range(len(labels)):\n",
    "        plt.scatter(x[i],y[i], color=c[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import pickle\n",
    "two_dim_model = pickle.load(open('../Datasets/word2vec2d.pkl', 'rb'))\n",
    "\n",
    "wv_labels = {}\n",
    "for vec, word in two_dim_model:\n",
    "    wv_labels[word] = vec\n",
    "    \n",
    "colors = ['blue' for i in range(len(wv_labels))]\n",
    "\n",
    "plot_values(wv_labels.values(), list(wv_labels.keys()), figsize = (16, 9), c = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have given you the 2D representation of different word vectors. Plot the word vectors for the words 'King', 'Queen', 'man', 'women', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest' in green color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_list = ['king', 'queen', 'man', 'woman', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest']\n",
    "wv_new_labels = {}\n",
    "for word in wv_list:\n",
    "    wv_new_labels[word] = wv_labels[word]\n",
    "\n",
    "colors = ['green' for i in range(len(wv_new_labels))]\n",
    "plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the word analogy question: \"Queen is to King, as Woman is to what?\"\n",
    "\n",
    "To answer this question, we aim to find what the difference between a \"King\" and \"Queen\" is, and apply that difference to a \"Woman\". If we try to put this mathematically, we can write:- \n",
    "$$\n",
    " Answer = Woman + King - Queen\n",
    "$$\n",
    "\n",
    "Compute the value of the vector on the right hand side of the above equation and plot the resulting vector in red in the same plot as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = wv_new_labels['woman']  + wv_new_labels['queen'] - wv_new_labels['king']\n",
    "\n",
    "wv_new_labels['answer1'] = answer\n",
    "\n",
    "colors = ['green' if word not in ['answer1'] else 'red' for word in wv_new_labels]\n",
    "\n",
    "plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the answer vector is very close to the vector of the word \"Man\"? Incidentally, \"Man\" is the right answer to the word analogy question! This is the power of Word2Vec representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this technique answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Germany is to France, as Paris is to?\n",
    "* Best is to Good, as Strongest is to?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2 Load pre-trained Word2Vec\n",
    "\n",
    "Let us now proceed to load the complete pretrained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../Datasets/GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Word2Vec representation\n",
    "\n",
    "Convert each document into average of the word2vec vectors of all valid words in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2vec_vector_size = 300\n",
    "\n",
    "'''\n",
    " This method converts documents to word vectors. It first checks if the word is valid according to our initial frequency \n",
    " threshold. Next, if it is, we add the precomputed word vectors together. If the word is valid, but we do not have a valid \n",
    " vector to represent the word, we add a random gaussian noise instead. Since we do not want to induce new noise each time,\n",
    " we store the same noise vector for training and test time in substitute_word_vecs variable.\n",
    "'''\n",
    "def convert_to_w2v(dataset, number_of_documents, substitute_word_vecs={}):\n",
    "    labels = np.zeros((number_of_documents, 1))\n",
    "    w2v_rep = np.zeros((number_of_documents, word2vec_vector_size))\n",
    "    \n",
    "    # Iterate over the dataset and split into words\n",
    "    i = 0\n",
    "    for label, class_name in enumerate(dataset):\n",
    "        for f in dataset[class_name]:\n",
    "            text = ' '.join(f).split(' ')\n",
    "            valid_count = 1\n",
    "            for word in text:\n",
    "                \n",
    "                # Check if word is valid or not according to original dataset pruning\n",
    "                if word in valid_words:\n",
    "                    try:\n",
    "                        w2v_rep[i] += model[word]\n",
    "                    except:\n",
    "                        '''The word isn't in our pretrained word-vectors, hence we add a random gaussian noise\n",
    "                         to account for this. We store the random vector we assigned to the word, and reuse \n",
    "                         the same vector during test time to ensure consistency.'''\n",
    "                        \n",
    "                        if word not in substitute_word_vecs.keys():\n",
    "                            substitute_word_vecs[word] = np.random.normal(-0.25, 0.25, word2vec_vector_size)\n",
    "                            \n",
    "                        w2v_rep[i] += substitute_word_vecs[word]\n",
    "                    \n",
    "                    valid_count += 1\n",
    "            \n",
    "            # Average\n",
    "            w2v_rep[i] = w2v_rep[i] / valid_count\n",
    "            \n",
    "            # Save label\n",
    "            labels[i] = label\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "    return w2v_rep, labels, substitute_word_vecs\n",
    "\n",
    "# Convert the train and test datasets into their word2vec representations\n",
    "train_w2v_set, train_w2v_labels, substitute_word_vecs = convert_to_w2v(train_set, n_train)\n",
    "test_w2v_set, test_w2v_labels,_ = convert_to_w2v(test_set, n_test, substitute_word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Document classification using Word2Vec\n",
    "\n",
    "For the test documents, use your favorite distance metric (Cosine, Eucilidean, etc.) to find similar news articles from your training set and classify using kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i, given_feature in enumerate(test_w2v_set):\n",
    "    print(\"Progress: {0:.04f}\".format((i+1)/len(test_w2v_set)), end=\"\\r\")\n",
    "    predicted_class = kNN_classify(3, train_w2v_set, train_w2v_labels, given_feature)\n",
    "    if predicted_class == int(test_w2v_labels[i]):\n",
    "        accuracy += 1\n",
    "print(accuracy / len(test_w2v_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency thresholds represents the minimum frequency a word must have to be considered relevant. Experiment with the following values of frequency threshold in your preprocessing step from section 1.2. Re-run all the codes with the new set of valid words and report the accuracies. Use the following values:\n",
    "\n",
    "`freq_thresh` = \n",
    "* 0\n",
    "* 1000\n",
    "\n",
    "Report the accuracies using both bag of words and word2vec features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 2.3, substitute_word_vectors is used as a proxy for a word vector which we do not know. We used a normal gaussian to represent this in that section. Experiment with the type of substitute word vectors used when you do not have a pretrained word vector. Use with the following:\n",
    "\n",
    "`substitute_word_vecs` : \n",
    "* np.ones\n",
    "* np.zeros\n",
    "\n",
    "Report the accuracies using only word2vec features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify news articles into their 20 news groups, experiment with three different classifiers with the following parameter choices.\n",
    "\n",
    "* K-NN \n",
    " ** K : 10, 50\n",
    " ** Distance Metric : Euclidean.\n",
    "\n",
    "* Naive Bayes\n",
    " ** Probability Distribution assumption : Gaussian.\n",
    " \n",
    "* Multiclass Linear classifier.\n",
    "\n",
    "Report the accuracies using both bag of words and word2vec features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your final accuracies on each classifier in terms of TP, FP, Precision, Recall and F-scores using both bag of words and word2vec features. \n",
    "\n",
    "You should have a table in the following format:\n",
    "\n",
    "Mthd | Bag  | of | Words | --- | Word |  2 | Vec | --- \n",
    "---- | -    | -  | -     |  -  |  -   |  - |  -  |  -\n",
    "Mtrc | TP   | FP | P     |  R  |  TP  | FP |  P  |  R  \n",
    "KNN  | -    | -  | -     |  -  | -    |  - | -   |  - \n",
    "NB   | -    | -  | -     |  -  | -    |  - | -   |- \n",
    "MLC  | -    | -  | -     |  -  | -    |  - | -   |- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
